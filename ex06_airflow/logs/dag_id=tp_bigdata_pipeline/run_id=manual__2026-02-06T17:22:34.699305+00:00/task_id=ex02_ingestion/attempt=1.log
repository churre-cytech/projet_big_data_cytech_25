[2026-02-06T17:23:52.796+0000] {local_task_job_runner.py:123} INFO - ::group::Pre task execution logs
[2026-02-06T17:23:52.811+0000] {taskinstance.py:2612} INFO - Dependencies all met for dep_context=non-requeueable deps ti=<TaskInstance: tp_bigdata_pipeline.ex02_ingestion manual__2026-02-06T17:22:34.699305+00:00 [queued]>
[2026-02-06T17:23:52.819+0000] {taskinstance.py:2612} INFO - Dependencies all met for dep_context=requeueable deps ti=<TaskInstance: tp_bigdata_pipeline.ex02_ingestion manual__2026-02-06T17:22:34.699305+00:00 [queued]>
[2026-02-06T17:23:52.819+0000] {taskinstance.py:2865} INFO - Starting attempt 1 of 1
[2026-02-06T17:23:52.842+0000] {taskinstance.py:2888} INFO - Executing <Task(BashOperator): ex02_ingestion> on 2026-02-06 17:22:34.699305+00:00
[2026-02-06T17:23:52.847+0000] {standard_task_runner.py:72} INFO - Started process 239 to run task
[2026-02-06T17:23:52.850+0000] {standard_task_runner.py:104} INFO - Running: ['airflow', 'tasks', 'run', 'tp_bigdata_pipeline', 'ex02_ingestion', 'manual__2026-02-06T17:22:34.699305+00:00', '--job-id', '60', '--raw', '--subdir', 'DAGS_FOLDER/airflow_pipeline.py', '--cfg-path', '/tmp/tmpc_j9u88n']
[2026-02-06T17:23:52.852+0000] {standard_task_runner.py:105} INFO - Job 60: Subtask ex02_ingestion
[2026-02-06T17:23:52.901+0000] {task_command.py:467} INFO - Running <TaskInstance: tp_bigdata_pipeline.ex02_ingestion manual__2026-02-06T17:22:34.699305+00:00 [running]> on host 4d21249f976f
[2026-02-06T17:23:52.998+0000] {taskinstance.py:3131} INFO - Exporting env vars: AIRFLOW_CTX_DAG_OWNER='airflow' AIRFLOW_CTX_DAG_ID='tp_bigdata_pipeline' AIRFLOW_CTX_TASK_ID='ex02_ingestion' AIRFLOW_CTX_EXECUTION_DATE='2026-02-06T17:22:34.699305+00:00' AIRFLOW_CTX_TRY_NUMBER='1' AIRFLOW_CTX_DAG_RUN_ID='manual__2026-02-06T17:22:34.699305+00:00'
[2026-02-06T17:23:52.998+0000] {taskinstance.py:731} INFO - ::endgroup::
[2026-02-06T17:23:53.015+0000] {subprocess.py:63} INFO - Tmp dir root location: /tmp
[2026-02-06T17:23:53.016+0000] {subprocess.py:75} INFO - Running command: ['/usr/bin/bash', '-c', '\n            set -euo pipefail\n            test -f /workspace/ex02_data_ingestion/target/scala-2.13/ex02-ingestion-assembly.jar\n            java --add-exports=java.base/sun.nio.ch=ALL-UNNAMED             --add-exports=java.base/sun.util.calendar=ALL-UNNAMED             -jar /workspace/ex02_data_ingestion/target/scala-2.13/ex02-ingestion-assembly.jar\n        ']
[2026-02-06T17:23:53.023+0000] {subprocess.py:86} INFO - Output:
[2026-02-06T17:23:53.579+0000] {subprocess.py:93} INFO - WARNING: sun.reflect.Reflection.getCallerClass is not supported. This will impact performance.
[2026-02-06T17:23:53.659+0000] {subprocess.py:93} INFO - ERROR StatusLogger Unrecognized format specifier [d]
[2026-02-06T17:23:53.660+0000] {subprocess.py:93} INFO - ERROR StatusLogger Unrecognized conversion specifier [d] starting at position 16 in conversion pattern.
[2026-02-06T17:23:53.660+0000] {subprocess.py:93} INFO - ERROR StatusLogger Unrecognized format specifier [thread]
[2026-02-06T17:23:53.661+0000] {subprocess.py:93} INFO - ERROR StatusLogger Unrecognized conversion specifier [thread] starting at position 25 in conversion pattern.
[2026-02-06T17:23:53.661+0000] {subprocess.py:93} INFO - ERROR StatusLogger Unrecognized format specifier [level]
[2026-02-06T17:23:53.661+0000] {subprocess.py:93} INFO - ERROR StatusLogger Unrecognized conversion specifier [level] starting at position 35 in conversion pattern.
[2026-02-06T17:23:53.661+0000] {subprocess.py:93} INFO - ERROR StatusLogger Unrecognized format specifier [logger]
[2026-02-06T17:23:53.661+0000] {subprocess.py:93} INFO - ERROR StatusLogger Unrecognized conversion specifier [logger] starting at position 47 in conversion pattern.
[2026-02-06T17:23:53.661+0000] {subprocess.py:93} INFO - ERROR StatusLogger Unrecognized format specifier [msg]
[2026-02-06T17:23:53.661+0000] {subprocess.py:93} INFO - ERROR StatusLogger Unrecognized conversion specifier [msg] starting at position 54 in conversion pattern.
[2026-02-06T17:23:53.661+0000] {subprocess.py:93} INFO - ERROR StatusLogger Unrecognized format specifier [n]
[2026-02-06T17:23:53.662+0000] {subprocess.py:93} INFO - ERROR StatusLogger Unrecognized conversion specifier [n] starting at position 56 in conversion pattern.
[2026-02-06T17:23:53.672+0000] {subprocess.py:93} INFO - ERROR StatusLogger Unrecognized format specifier [d]
[2026-02-06T17:23:53.673+0000] {subprocess.py:93} INFO - ERROR StatusLogger Unrecognized conversion specifier [d] starting at position 16 in conversion pattern.
[2026-02-06T17:23:53.673+0000] {subprocess.py:93} INFO - ERROR StatusLogger Unrecognized format specifier [thread]
[2026-02-06T17:23:53.673+0000] {subprocess.py:93} INFO - ERROR StatusLogger Unrecognized conversion specifier [thread] starting at position 25 in conversion pattern.
[2026-02-06T17:23:53.673+0000] {subprocess.py:93} INFO - ERROR StatusLogger Unrecognized format specifier [level]
[2026-02-06T17:23:53.673+0000] {subprocess.py:93} INFO - ERROR StatusLogger Unrecognized conversion specifier [level] starting at position 35 in conversion pattern.
[2026-02-06T17:23:53.673+0000] {subprocess.py:93} INFO - ERROR StatusLogger Unrecognized format specifier [logger]
[2026-02-06T17:23:53.673+0000] {subprocess.py:93} INFO - ERROR StatusLogger Unrecognized conversion specifier [logger] starting at position 47 in conversion pattern.
[2026-02-06T17:23:53.673+0000] {subprocess.py:93} INFO - ERROR StatusLogger Unrecognized format specifier [msg]
[2026-02-06T17:23:53.673+0000] {subprocess.py:93} INFO - ERROR StatusLogger Unrecognized conversion specifier [msg] starting at position 54 in conversion pattern.
[2026-02-06T17:23:53.673+0000] {subprocess.py:93} INFO - ERROR StatusLogger Unrecognized format specifier [n]
[2026-02-06T17:23:53.673+0000] {subprocess.py:93} INFO - ERROR StatusLogger Unrecognized conversion specifier [n] starting at position 56 in conversion pattern.
[2026-02-06T17:23:53.872+0000] {subprocess.py:93} INFO - ERROR StatusLogger Unrecognized format specifier [d]
[2026-02-06T17:23:53.872+0000] {subprocess.py:93} INFO - ERROR StatusLogger Unrecognized conversion specifier [d] starting at position 16 in conversion pattern.
[2026-02-06T17:23:53.872+0000] {subprocess.py:93} INFO - ERROR StatusLogger Unrecognized format specifier [thread]
[2026-02-06T17:23:53.873+0000] {subprocess.py:93} INFO - ERROR StatusLogger Unrecognized conversion specifier [thread] starting at position 25 in conversion pattern.
[2026-02-06T17:23:53.873+0000] {subprocess.py:93} INFO - ERROR StatusLogger Unrecognized format specifier [level]
[2026-02-06T17:23:53.873+0000] {subprocess.py:93} INFO - ERROR StatusLogger Unrecognized conversion specifier [level] starting at position 35 in conversion pattern.
[2026-02-06T17:23:53.873+0000] {subprocess.py:93} INFO - ERROR StatusLogger Unrecognized format specifier [logger]
[2026-02-06T17:23:53.873+0000] {subprocess.py:93} INFO - ERROR StatusLogger Unrecognized conversion specifier [logger] starting at position 47 in conversion pattern.
[2026-02-06T17:23:53.873+0000] {subprocess.py:93} INFO - ERROR StatusLogger Unrecognized format specifier [msg]
[2026-02-06T17:23:53.873+0000] {subprocess.py:93} INFO - ERROR StatusLogger Unrecognized conversion specifier [msg] starting at position 54 in conversion pattern.
[2026-02-06T17:23:53.873+0000] {subprocess.py:93} INFO - ERROR StatusLogger Unrecognized format specifier [n]
[2026-02-06T17:23:53.873+0000] {subprocess.py:93} INFO - ERROR StatusLogger Unrecognized conversion specifier [n] starting at position 56 in conversion pattern.
[2026-02-06T17:23:53.873+0000] {subprocess.py:93} INFO - Using Spark's default log4j profile: org/apache/spark/log4j2-defaults.properties
[2026-02-06T17:24:00.855+0000] {subprocess.py:93} INFO - %d [%thread] %-5level %logger - %msg%n org.apache.hadoop.fs.UnsupportedFileSystemException: No FileSystem for scheme "file"
[2026-02-06T17:24:00.855+0000] {subprocess.py:93} INFO - 	at org.apache.hadoop.fs.FileSystem.getFileSystemClass(FileSystem.java:3443) ~[ex02-ingestion-assembly.jar:1.0.0]
[2026-02-06T17:24:00.856+0000] {subprocess.py:93} INFO - 	at org.apache.hadoop.fs.FileSystem.createFileSystem(FileSystem.java:3466) ~[ex02-ingestion-assembly.jar:1.0.0]
[2026-02-06T17:24:00.856+0000] {subprocess.py:93} INFO - 	at org.apache.hadoop.fs.FileSystem.access$300(FileSystem.java:174) ~[ex02-ingestion-assembly.jar:1.0.0]
[2026-02-06T17:24:00.856+0000] {subprocess.py:93} INFO - 	at org.apache.hadoop.fs.FileSystem$Cache.getInternal(FileSystem.java:3574) ~[ex02-ingestion-assembly.jar:1.0.0]
[2026-02-06T17:24:00.857+0000] {subprocess.py:93} INFO - 	at org.apache.hadoop.fs.FileSystem$Cache.get(FileSystem.java:3521) ~[ex02-ingestion-assembly.jar:1.0.0]
[2026-02-06T17:24:00.857+0000] {subprocess.py:93} INFO - 	at org.apache.hadoop.fs.FileSystem.get(FileSystem.java:540) ~[ex02-ingestion-assembly.jar:1.0.0]
[2026-02-06T17:24:00.857+0000] {subprocess.py:93} INFO - 	at org.apache.hadoop.fs.FileSystem.getLocal(FileSystem.java:496) ~[ex02-ingestion-assembly.jar:1.0.0]
[2026-02-06T17:24:00.858+0000] {subprocess.py:93} INFO - 	at org.apache.hadoop.fs.LocalDirAllocator$AllocatorPerContext.confChanged(LocalDirAllocator.java:316) ~[ex02-ingestion-assembly.jar:1.0.0]
[2026-02-06T17:24:00.858+0000] {subprocess.py:93} INFO - 	at org.apache.hadoop.fs.LocalDirAllocator$AllocatorPerContext.getLocalPathForWrite(LocalDirAllocator.java:393) ~[ex02-ingestion-assembly.jar:1.0.0]
[2026-02-06T17:24:00.858+0000] {subprocess.py:93} INFO - 	at org.apache.hadoop.fs.LocalDirAllocator.getLocalPathForWrite(LocalDirAllocator.java:165) ~[ex02-ingestion-assembly.jar:1.0.0]
[2026-02-06T17:24:00.858+0000] {subprocess.py:93} INFO - 	at org.apache.hadoop.fs.LocalDirAllocator.getLocalPathForWrite(LocalDirAllocator.java:146) ~[ex02-ingestion-assembly.jar:1.0.0]
[2026-02-06T17:24:00.858+0000] {subprocess.py:93} INFO - 	at org.apache.hadoop.fs.s3a.S3AFileSystem.createTmpFileForWrite(S3AFileSystem.java:1282) ~[ex02-ingestion-assembly.jar:1.0.0]
[2026-02-06T17:24:00.858+0000] {subprocess.py:93} INFO - 	at org.apache.hadoop.fs.s3a.S3ADataBlocks$DiskBlockFactory.create(S3ADataBlocks.java:816) ~[ex02-ingestion-assembly.jar:1.0.0]
[2026-02-06T17:24:00.858+0000] {subprocess.py:93} INFO - 	at org.apache.hadoop.fs.s3a.S3ABlockOutputStream.createBlockIfNeeded(S3ABlockOutputStream.java:211) ~[ex02-ingestion-assembly.jar:1.0.0]
[2026-02-06T17:24:00.858+0000] {subprocess.py:93} INFO - 	at org.apache.hadoop.fs.s3a.S3ABlockOutputStream.<init>(S3ABlockOutputStream.java:188) ~[ex02-ingestion-assembly.jar:1.0.0]
[2026-02-06T17:24:00.859+0000] {subprocess.py:93} INFO - 	at org.apache.hadoop.fs.s3a.S3AFileSystem.innerCreateFile(S3AFileSystem.java:1727) ~[ex02-ingestion-assembly.jar:1.0.0]
[2026-02-06T17:24:00.859+0000] {subprocess.py:93} INFO - 	at org.apache.hadoop.fs.s3a.S3AFileSystem.lambda$create$6(S3AFileSystem.java:1646) ~[ex02-ingestion-assembly.jar:1.0.0]
[2026-02-06T17:24:00.859+0000] {subprocess.py:93} INFO - 	at org.apache.hadoop.fs.statistics.impl.IOStatisticsBinding.lambda$trackDurationOfOperation$5(IOStatisticsBinding.java:499) ~[ex02-ingestion-assembly.jar:1.0.0]
[2026-02-06T17:24:00.859+0000] {subprocess.py:93} INFO - 	at org.apache.hadoop.fs.statistics.impl.IOStatisticsBinding.trackDuration(IOStatisticsBinding.java:444) ~[ex02-ingestion-assembly.jar:1.0.0]
[2026-02-06T17:24:00.859+0000] {subprocess.py:93} INFO - 	at org.apache.hadoop.fs.s3a.S3AFileSystem.trackDurationAndSpan(S3AFileSystem.java:2337) ~[ex02-ingestion-assembly.jar:1.0.0]
[2026-02-06T17:24:00.859+0000] {subprocess.py:93} INFO - 	at org.apache.hadoop.fs.s3a.S3AFileSystem.trackDurationAndSpan(S3AFileSystem.java:2356) ~[ex02-ingestion-assembly.jar:1.0.0]
[2026-02-06T17:24:00.859+0000] {subprocess.py:93} INFO - 	at org.apache.hadoop.fs.s3a.S3AFileSystem.create(S3AFileSystem.java:1645) ~[ex02-ingestion-assembly.jar:1.0.0]
[2026-02-06T17:24:00.860+0000] {subprocess.py:93} INFO - 	at org.apache.hadoop.fs.FileSystem.create(FileSystem.java:1195) ~[ex02-ingestion-assembly.jar:1.0.0]
[2026-02-06T17:24:00.860+0000] {subprocess.py:93} INFO - 	at org.apache.hadoop.fs.FileSystem.create(FileSystem.java:1175) ~[ex02-ingestion-assembly.jar:1.0.0]
[2026-02-06T17:24:00.860+0000] {subprocess.py:93} INFO - 	at org.apache.parquet.hadoop.util.HadoopOutputFile.create(HadoopOutputFile.java:74) ~[ex02-ingestion-assembly.jar:1.0.0]
[2026-02-06T17:24:00.860+0000] {subprocess.py:93} INFO - 	at org.apache.parquet.hadoop.ParquetFileWriter.<init>(ParquetFileWriter.java:347) ~[ex02-ingestion-assembly.jar:1.0.0]
[2026-02-06T17:24:00.861+0000] {subprocess.py:93} INFO - 	at org.apache.parquet.hadoop.ParquetFileWriter.<init>(ParquetFileWriter.java:314) ~[ex02-ingestion-assembly.jar:1.0.0]
[2026-02-06T17:24:00.861+0000] {subprocess.py:93} INFO - 	at org.apache.parquet.hadoop.ParquetOutputFormat.getRecordWriter(ParquetOutputFormat.java:484) ~[ex02-ingestion-assembly.jar:1.0.0]
[2026-02-06T17:24:00.861+0000] {subprocess.py:93} INFO - 	at org.apache.parquet.hadoop.ParquetOutputFormat.getRecordWriter(ParquetOutputFormat.java:422) ~[ex02-ingestion-assembly.jar:1.0.0]
[2026-02-06T17:24:00.861+0000] {subprocess.py:93} INFO - 	at org.apache.parquet.hadoop.ParquetOutputFormat.getRecordWriter(ParquetOutputFormat.java:411) ~[ex02-ingestion-assembly.jar:1.0.0]
[2026-02-06T17:24:00.861+0000] {subprocess.py:93} INFO - 	at org.apache.spark.sql.execution.datasources.parquet.ParquetOutputWriter.<init>(ParquetOutputWriter.scala:36) ~[ex02-ingestion-assembly.jar:1.0.0]
[2026-02-06T17:24:00.861+0000] {subprocess.py:93} INFO - 	at org.apache.spark.sql.execution.datasources.parquet.ParquetUtils$$anon$1.newInstance(ParquetUtils.scala:490) ~[ex02-ingestion-assembly.jar:1.0.0]
[2026-02-06T17:24:00.861+0000] {subprocess.py:93} INFO - 	at org.apache.spark.sql.execution.datasources.SingleDirectoryDataWriter.newOutputWriter(FileFormatDataWriter.scala:161) ~[ex02-ingestion-assembly.jar:1.0.0]
[2026-02-06T17:24:00.861+0000] {subprocess.py:93} INFO - 	at org.apache.spark.sql.execution.datasources.SingleDirectoryDataWriter.<init>(FileFormatDataWriter.scala:146) ~[ex02-ingestion-assembly.jar:1.0.0]
[2026-02-06T17:24:00.861+0000] {subprocess.py:93} INFO - 	at org.apache.spark.sql.execution.datasources.FileFormatWriter$.executeTask(FileFormatWriter.scala:389) ~[ex02-ingestion-assembly.jar:1.0.0]
[2026-02-06T17:24:00.861+0000] {subprocess.py:93} INFO - 	at org.apache.spark.sql.execution.datasources.WriteFilesExec.$anonfun$doExecuteWrite$1(WriteFiles.scala:100) ~[ex02-ingestion-assembly.jar:1.0.0]
[2026-02-06T17:24:00.861+0000] {subprocess.py:93} INFO - 	at org.apache.spark.rdd.RDD.$anonfun$mapPartitionsInternal$2(RDD.scala:893) ~[ex02-ingestion-assembly.jar:1.0.0]
[2026-02-06T17:24:00.861+0000] {subprocess.py:93} INFO - 	at org.apache.spark.rdd.RDD.$anonfun$mapPartitionsInternal$2$adapted(RDD.scala:893) ~[ex02-ingestion-assembly.jar:1.0.0]
[2026-02-06T17:24:00.862+0000] {subprocess.py:93} INFO - 	at org.apache.spark.rdd.MapPartitionsRDD.compute(MapPartitionsRDD.scala:52) ~[ex02-ingestion-assembly.jar:1.0.0]
[2026-02-06T17:24:00.862+0000] {subprocess.py:93} INFO - 	at org.apache.spark.rdd.RDD.computeOrReadCheckpoint(RDD.scala:367) ~[ex02-ingestion-assembly.jar:1.0.0]
[2026-02-06T17:24:00.862+0000] {subprocess.py:93} INFO - 	at org.apache.spark.rdd.RDD.iterator(RDD.scala:331) ~[ex02-ingestion-assembly.jar:1.0.0]
[2026-02-06T17:24:00.862+0000] {subprocess.py:93} INFO - 	at org.apache.spark.scheduler.ResultTask.runTask(ResultTask.scala:93) ~[ex02-ingestion-assembly.jar:1.0.0]
[2026-02-06T17:24:00.862+0000] {subprocess.py:93} INFO - 	at org.apache.spark.TaskContext.runTaskWithListeners(TaskContext.scala:166) ~[ex02-ingestion-assembly.jar:1.0.0]
[2026-02-06T17:24:00.862+0000] {subprocess.py:93} INFO - 	at org.apache.spark.scheduler.Task.run(Task.scala:141) ~[ex02-ingestion-assembly.jar:1.0.0]
[2026-02-06T17:24:00.862+0000] {subprocess.py:93} INFO - 	at org.apache.spark.executor.Executor$TaskRunner.$anonfun$run$4(Executor.scala:620) ~[ex02-ingestion-assembly.jar:1.0.0]
[2026-02-06T17:24:00.862+0000] {subprocess.py:93} INFO - 	at org.apache.spark.util.SparkErrorUtils.tryWithSafeFinally(SparkErrorUtils.scala:64) ~[ex02-ingestion-assembly.jar:1.0.0]
[2026-02-06T17:24:00.862+0000] {subprocess.py:93} INFO - 	at org.apache.spark.util.SparkErrorUtils.tryWithSafeFinally$(SparkErrorUtils.scala:61) ~[ex02-ingestion-assembly.jar:1.0.0]
[2026-02-06T17:24:00.862+0000] {subprocess.py:93} INFO - 	at org.apache.spark.util.Utils$.tryWithSafeFinally(Utils.scala:94) ~[ex02-ingestion-assembly.jar:1.0.0]
[2026-02-06T17:24:00.863+0000] {subprocess.py:93} INFO - 	at org.apache.spark.executor.Executor$TaskRunner.run(Executor.scala:623) [ex02-ingestion-assembly.jar:1.0.0]
[2026-02-06T17:24:00.863+0000] {subprocess.py:93} INFO - 	at java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1136) [?:?]
[2026-02-06T17:24:00.863+0000] {subprocess.py:93} INFO - 	at java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:635) [?:?]
[2026-02-06T17:24:00.863+0000] {subprocess.py:93} INFO - 	at java.lang.Thread.run(Thread.java:840) [?:?]
[2026-02-06T17:24:00.931+0000] {subprocess.py:93} INFO - %d [%thread] %-5level %logger - %msg%n%d [%thread] %-5level %logger - %msg%n org.apache.spark.SparkException: Job aborted due to stage failure: Task 0 in stage 1.0 failed 1 times, most recent failure: Lost task 0.0 in stage 1.0 (TID 1) (4d21249f976f executor driver): org.apache.hadoop.fs.UnsupportedFileSystemException: No FileSystem for scheme "file"
[2026-02-06T17:24:00.933+0000] {subprocess.py:93} INFO - 	at org.apache.hadoop.fs.FileSystem.getFileSystemClass(FileSystem.java:3443)
[2026-02-06T17:24:00.934+0000] {subprocess.py:93} INFO - 	at org.apache.hadoop.fs.FileSystem.createFileSystem(FileSystem.java:3466)
[2026-02-06T17:24:00.934+0000] {subprocess.py:93} INFO - 	at org.apache.hadoop.fs.FileSystem.access$300(FileSystem.java:174)
[2026-02-06T17:24:00.935+0000] {subprocess.py:93} INFO - 	at org.apache.hadoop.fs.FileSystem$Cache.getInternal(FileSystem.java:3574)
[2026-02-06T17:24:00.935+0000] {subprocess.py:93} INFO - 	at org.apache.hadoop.fs.FileSystem$Cache.get(FileSystem.java:3521)
[2026-02-06T17:24:00.936+0000] {subprocess.py:93} INFO - 	at org.apache.hadoop.fs.FileSystem.get(FileSystem.java:540)
[2026-02-06T17:24:00.936+0000] {subprocess.py:93} INFO - 	at org.apache.hadoop.fs.FileSystem.getLocal(FileSystem.java:496)
[2026-02-06T17:24:00.936+0000] {subprocess.py:93} INFO - 	at org.apache.hadoop.fs.LocalDirAllocator$AllocatorPerContext.confChanged(LocalDirAllocator.java:316)
[2026-02-06T17:24:00.937+0000] {subprocess.py:93} INFO - 	at org.apache.hadoop.fs.LocalDirAllocator$AllocatorPerContext.getLocalPathForWrite(LocalDirAllocator.java:393)
[2026-02-06T17:24:00.937+0000] {subprocess.py:93} INFO - 	at org.apache.hadoop.fs.LocalDirAllocator.getLocalPathForWrite(LocalDirAllocator.java:165)
[2026-02-06T17:24:00.937+0000] {subprocess.py:93} INFO - 	at org.apache.hadoop.fs.LocalDirAllocator.getLocalPathForWrite(LocalDirAllocator.java:146)
[2026-02-06T17:24:00.937+0000] {subprocess.py:93} INFO - 	at org.apache.hadoop.fs.s3a.S3AFileSystem.createTmpFileForWrite(S3AFileSystem.java:1282)
[2026-02-06T17:24:00.938+0000] {subprocess.py:93} INFO - 	at org.apache.hadoop.fs.s3a.S3ADataBlocks$DiskBlockFactory.create(S3ADataBlocks.java:816)
[2026-02-06T17:24:00.938+0000] {subprocess.py:93} INFO - 	at org.apache.hadoop.fs.s3a.S3ABlockOutputStream.createBlockIfNeeded(S3ABlockOutputStream.java:211)
[2026-02-06T17:24:00.938+0000] {subprocess.py:93} INFO - 	at org.apache.hadoop.fs.s3a.S3ABlockOutputStream.<init>(S3ABlockOutputStream.java:188)
[2026-02-06T17:24:00.938+0000] {subprocess.py:93} INFO - 	at org.apache.hadoop.fs.s3a.S3AFileSystem.innerCreateFile(S3AFileSystem.java:1727)
[2026-02-06T17:24:00.938+0000] {subprocess.py:93} INFO - 	at org.apache.hadoop.fs.s3a.S3AFileSystem.lambda$create$6(S3AFileSystem.java:1646)
[2026-02-06T17:24:00.938+0000] {subprocess.py:93} INFO - 	at org.apache.hadoop.fs.statistics.impl.IOStatisticsBinding.lambda$trackDurationOfOperation$5(IOStatisticsBinding.java:499)
[2026-02-06T17:24:00.939+0000] {subprocess.py:93} INFO - 	at org.apache.hadoop.fs.statistics.impl.IOStatisticsBinding.trackDuration(IOStatisticsBinding.java:444)
[2026-02-06T17:24:00.939+0000] {subprocess.py:93} INFO - 	at org.apache.hadoop.fs.s3a.S3AFileSystem.trackDurationAndSpan(S3AFileSystem.java:2337)
[2026-02-06T17:24:00.939+0000] {subprocess.py:93} INFO - 	at org.apache.hadoop.fs.s3a.S3AFileSystem.trackDurationAndSpan(S3AFileSystem.java:2356)
[2026-02-06T17:24:00.939+0000] {subprocess.py:93} INFO - 	at org.apache.hadoop.fs.s3a.S3AFileSystem.create(S3AFileSystem.java:1645)
[2026-02-06T17:24:00.939+0000] {subprocess.py:93} INFO - 	at org.apache.hadoop.fs.FileSystem.create(FileSystem.java:1195)
[2026-02-06T17:24:00.939+0000] {subprocess.py:93} INFO - 	at org.apache.hadoop.fs.FileSystem.create(FileSystem.java:1175)
[2026-02-06T17:24:00.939+0000] {subprocess.py:93} INFO - 	at org.apache.parquet.hadoop.util.HadoopOutputFile.create(HadoopOutputFile.java:74)
[2026-02-06T17:24:00.940+0000] {subprocess.py:93} INFO - 	at org.apache.parquet.hadoop.ParquetFileWriter.<init>(ParquetFileWriter.java:347)
[2026-02-06T17:24:00.940+0000] {subprocess.py:93} INFO - 	at org.apache.parquet.hadoop.ParquetFileWriter.<init>(ParquetFileWriter.java:314)
[2026-02-06T17:24:00.940+0000] {subprocess.py:93} INFO - 	at org.apache.parquet.hadoop.ParquetOutputFormat.getRecordWriter(ParquetOutputFormat.java:484)
[2026-02-06T17:24:00.940+0000] {subprocess.py:93} INFO - 	at org.apache.parquet.hadoop.ParquetOutputFormat.getRecordWriter(ParquetOutputFormat.java:422)
[2026-02-06T17:24:00.940+0000] {subprocess.py:93} INFO - 	at org.apache.parquet.hadoop.ParquetOutputFormat.getRecordWriter(ParquetOutputFormat.java:411)
[2026-02-06T17:24:00.940+0000] {subprocess.py:93} INFO - 	at org.apache.spark.sql.execution.datasources.parquet.ParquetOutputWriter.<init>(ParquetOutputWriter.scala:36)
[2026-02-06T17:24:00.940+0000] {subprocess.py:93} INFO - 	at org.apache.spark.sql.execution.datasources.parquet.ParquetUtils$$anon$1.newInstance(ParquetUtils.scala:490)
[2026-02-06T17:24:00.940+0000] {subprocess.py:93} INFO - 	at org.apache.spark.sql.execution.datasources.SingleDirectoryDataWriter.newOutputWriter(FileFormatDataWriter.scala:161)
[2026-02-06T17:24:00.940+0000] {subprocess.py:93} INFO - 	at org.apache.spark.sql.execution.datasources.SingleDirectoryDataWriter.<init>(FileFormatDataWriter.scala:146)
[2026-02-06T17:24:00.940+0000] {subprocess.py:93} INFO - 	at org.apache.spark.sql.execution.datasources.FileFormatWriter$.executeTask(FileFormatWriter.scala:389)
[2026-02-06T17:24:00.941+0000] {subprocess.py:93} INFO - 	at org.apache.spark.sql.execution.datasources.WriteFilesExec.$anonfun$doExecuteWrite$1(WriteFiles.scala:100)
[2026-02-06T17:24:00.941+0000] {subprocess.py:93} INFO - 	at org.apache.spark.rdd.RDD.$anonfun$mapPartitionsInternal$2(RDD.scala:893)
[2026-02-06T17:24:00.941+0000] {subprocess.py:93} INFO - 	at org.apache.spark.rdd.RDD.$anonfun$mapPartitionsInternal$2$adapted(RDD.scala:893)
[2026-02-06T17:24:00.941+0000] {subprocess.py:93} INFO - 	at org.apache.spark.rdd.MapPartitionsRDD.compute(MapPartitionsRDD.scala:52)
[2026-02-06T17:24:00.941+0000] {subprocess.py:93} INFO - 	at org.apache.spark.rdd.RDD.computeOrReadCheckpoint(RDD.scala:367)
[2026-02-06T17:24:00.941+0000] {subprocess.py:93} INFO - 	at org.apache.spark.rdd.RDD.iterator(RDD.scala:331)
[2026-02-06T17:24:00.941+0000] {subprocess.py:93} INFO - 	at org.apache.spark.scheduler.ResultTask.runTask(ResultTask.scala:93)
[2026-02-06T17:24:00.941+0000] {subprocess.py:93} INFO - 	at org.apache.spark.TaskContext.runTaskWithListeners(TaskContext.scala:166)
[2026-02-06T17:24:00.941+0000] {subprocess.py:93} INFO - 	at org.apache.spark.scheduler.Task.run(Task.scala:141)
[2026-02-06T17:24:00.941+0000] {subprocess.py:93} INFO - 	at org.apache.spark.executor.Executor$TaskRunner.$anonfun$run$4(Executor.scala:620)
[2026-02-06T17:24:00.941+0000] {subprocess.py:93} INFO - 	at org.apache.spark.util.SparkErrorUtils.tryWithSafeFinally(SparkErrorUtils.scala:64)
[2026-02-06T17:24:00.942+0000] {subprocess.py:93} INFO - 	at org.apache.spark.util.SparkErrorUtils.tryWithSafeFinally$(SparkErrorUtils.scala:61)
[2026-02-06T17:24:00.942+0000] {subprocess.py:93} INFO - 	at org.apache.spark.util.Utils$.tryWithSafeFinally(Utils.scala:94)
[2026-02-06T17:24:00.942+0000] {subprocess.py:93} INFO - 	at org.apache.spark.executor.Executor$TaskRunner.run(Executor.scala:623)
[2026-02-06T17:24:00.942+0000] {subprocess.py:93} INFO - 	at java.base/java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1136)
[2026-02-06T17:24:00.942+0000] {subprocess.py:93} INFO - 	at java.base/java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:635)
[2026-02-06T17:24:00.942+0000] {subprocess.py:93} INFO - 	at java.base/java.lang.Thread.run(Thread.java:840)
[2026-02-06T17:24:00.942+0000] {subprocess.py:93} INFO - 
[2026-02-06T17:24:00.942+0000] {subprocess.py:93} INFO - Driver stacktrace:
[2026-02-06T17:24:00.942+0000] {subprocess.py:93} INFO - 	at org.apache.spark.scheduler.DAGScheduler.failJobAndIndependentStages(DAGScheduler.scala:2856) ~[ex02-ingestion-assembly.jar:1.0.0]
[2026-02-06T17:24:00.942+0000] {subprocess.py:93} INFO - 	at org.apache.spark.scheduler.DAGScheduler.$anonfun$abortStage$2(DAGScheduler.scala:2792) ~[ex02-ingestion-assembly.jar:1.0.0]
[2026-02-06T17:24:00.942+0000] {subprocess.py:93} INFO - 	at org.apache.spark.scheduler.DAGScheduler.$anonfun$abortStage$2$adapted(DAGScheduler.scala:2791) ~[ex02-ingestion-assembly.jar:1.0.0]
[2026-02-06T17:24:00.942+0000] {subprocess.py:93} INFO - 	at scala.collection.immutable.List.foreach(List.scala:323) ~[ex02-ingestion-assembly.jar:1.0.0]
[2026-02-06T17:24:00.943+0000] {subprocess.py:93} INFO - 	at org.apache.spark.scheduler.DAGScheduler.abortStage(DAGScheduler.scala:2791) ~[ex02-ingestion-assembly.jar:1.0.0]
[2026-02-06T17:24:00.943+0000] {subprocess.py:93} INFO - 	at org.apache.spark.scheduler.DAGScheduler.$anonfun$handleTaskSetFailed$1(DAGScheduler.scala:1247) ~[ex02-ingestion-assembly.jar:1.0.0]
[2026-02-06T17:24:00.943+0000] {subprocess.py:93} INFO - 	at org.apache.spark.scheduler.DAGScheduler.$anonfun$handleTaskSetFailed$1$adapted(DAGScheduler.scala:1247) ~[ex02-ingestion-assembly.jar:1.0.0]
[2026-02-06T17:24:00.943+0000] {subprocess.py:93} INFO - 	at scala.Option.foreach(Option.scala:437) ~[ex02-ingestion-assembly.jar:1.0.0]
[2026-02-06T17:24:00.943+0000] {subprocess.py:93} INFO - 	at org.apache.spark.scheduler.DAGScheduler.handleTaskSetFailed(DAGScheduler.scala:1247) ~[ex02-ingestion-assembly.jar:1.0.0]
[2026-02-06T17:24:00.943+0000] {subprocess.py:93} INFO - 	at org.apache.spark.scheduler.DAGSchedulerEventProcessLoop.doOnReceive(DAGScheduler.scala:3060) ~[ex02-ingestion-assembly.jar:1.0.0]
[2026-02-06T17:24:00.943+0000] {subprocess.py:93} INFO - 	at org.apache.spark.scheduler.DAGSchedulerEventProcessLoop.onReceive(DAGScheduler.scala:2994) ~[ex02-ingestion-assembly.jar:1.0.0]
[2026-02-06T17:24:00.943+0000] {subprocess.py:93} INFO - 	at org.apache.spark.scheduler.DAGSchedulerEventProcessLoop.onReceive(DAGScheduler.scala:2983) ~[ex02-ingestion-assembly.jar:1.0.0]
[2026-02-06T17:24:00.943+0000] {subprocess.py:93} INFO - 	at org.apache.spark.util.EventLoop$$anon$1.run(EventLoop.scala:49) ~[ex02-ingestion-assembly.jar:1.0.0]
[2026-02-06T17:24:00.943+0000] {subprocess.py:93} INFO - 	at org.apache.spark.scheduler.DAGScheduler.runJob(DAGScheduler.scala:989) ~[ex02-ingestion-assembly.jar:1.0.0]
[2026-02-06T17:24:00.943+0000] {subprocess.py:93} INFO - 	at org.apache.spark.SparkContext.runJob(SparkContext.scala:2393) ~[ex02-ingestion-assembly.jar:1.0.0]
[2026-02-06T17:24:00.943+0000] {subprocess.py:93} INFO - 	at org.apache.spark.sql.execution.datasources.FileFormatWriter$.$anonfun$executeWrite$4(FileFormatWriter.scala:307) ~[ex02-ingestion-assembly.jar:1.0.0]
[2026-02-06T17:24:00.943+0000] {subprocess.py:93} INFO - 	at org.apache.spark.sql.execution.datasources.FileFormatWriter$.writeAndCommit(FileFormatWriter.scala:271) ~[ex02-ingestion-assembly.jar:1.0.0]
[2026-02-06T17:24:00.943+0000] {subprocess.py:93} INFO - 	at org.apache.spark.sql.execution.datasources.FileFormatWriter$.executeWrite(FileFormatWriter.scala:304) ~[ex02-ingestion-assembly.jar:1.0.0]
[2026-02-06T17:24:00.944+0000] {subprocess.py:93} INFO - 	at org.apache.spark.sql.execution.datasources.FileFormatWriter$.write(FileFormatWriter.scala:190) ~[ex02-ingestion-assembly.jar:1.0.0]
[2026-02-06T17:24:00.944+0000] {subprocess.py:93} INFO - 	at org.apache.spark.sql.execution.datasources.InsertIntoHadoopFsRelationCommand.run(InsertIntoHadoopFsRelationCommand.scala:190) ~[ex02-ingestion-assembly.jar:1.0.0]
[2026-02-06T17:24:00.944+0000] {subprocess.py:93} INFO - 	at org.apache.spark.sql.execution.command.DataWritingCommandExec.sideEffectResult$lzycompute(commands.scala:113) ~[ex02-ingestion-assembly.jar:1.0.0]
[2026-02-06T17:24:00.945+0000] {subprocess.py:93} INFO - 	at org.apache.spark.sql.execution.command.DataWritingCommandExec.sideEffectResult(commands.scala:111) ~[ex02-ingestion-assembly.jar:1.0.0]
[2026-02-06T17:24:00.945+0000] {subprocess.py:93} INFO - 	at org.apache.spark.sql.execution.command.DataWritingCommandExec.executeCollect(commands.scala:125) ~[ex02-ingestion-assembly.jar:1.0.0]
[2026-02-06T17:24:00.946+0000] {subprocess.py:93} INFO - 	at org.apache.spark.sql.execution.QueryExecution$$anonfun$eagerlyExecuteCommands$1.$anonfun$applyOrElse$1(QueryExecution.scala:107) ~[ex02-ingestion-assembly.jar:1.0.0]
[2026-02-06T17:24:00.946+0000] {subprocess.py:93} INFO - 	at org.apache.spark.sql.execution.SQLExecution$.$anonfun$withNewExecutionId$6(SQLExecution.scala:125) ~[ex02-ingestion-assembly.jar:1.0.0]
[2026-02-06T17:24:00.946+0000] {subprocess.py:93} INFO - 	at org.apache.spark.sql.execution.SQLExecution$.withSQLConfPropagated(SQLExecution.scala:201) ~[ex02-ingestion-assembly.jar:1.0.0]
[2026-02-06T17:24:00.946+0000] {subprocess.py:93} INFO - 	at org.apache.spark.sql.execution.SQLExecution$.$anonfun$withNewExecutionId$1(SQLExecution.scala:108) ~[ex02-ingestion-assembly.jar:1.0.0]
[2026-02-06T17:24:00.946+0000] {subprocess.py:93} INFO - 	at org.apache.spark.sql.SparkSession.withActive(SparkSession.scala:900) ~[ex02-ingestion-assembly.jar:1.0.0]
[2026-02-06T17:24:00.946+0000] {subprocess.py:93} INFO - 	at org.apache.spark.sql.execution.SQLExecution$.withNewExecutionId(SQLExecution.scala:66) ~[ex02-ingestion-assembly.jar:1.0.0]
[2026-02-06T17:24:00.947+0000] {subprocess.py:93} INFO - 	at org.apache.spark.sql.execution.QueryExecution$$anonfun$eagerlyExecuteCommands$1.applyOrElse(QueryExecution.scala:107) ~[ex02-ingestion-assembly.jar:1.0.0]
[2026-02-06T17:24:00.947+0000] {subprocess.py:93} INFO - 	at org.apache.spark.sql.execution.QueryExecution$$anonfun$eagerlyExecuteCommands$1.applyOrElse(QueryExecution.scala:98) ~[ex02-ingestion-assembly.jar:1.0.0]
[2026-02-06T17:24:00.947+0000] {subprocess.py:93} INFO - 	at org.apache.spark.sql.catalyst.trees.TreeNode.$anonfun$transformDownWithPruning$1(TreeNode.scala:461) ~[ex02-ingestion-assembly.jar:1.0.0]
[2026-02-06T17:24:00.947+0000] {subprocess.py:93} INFO - 	at org.apache.spark.sql.catalyst.trees.CurrentOrigin$.withOrigin(origin.scala:76) [ex02-ingestion-assembly.jar:1.0.0]
[2026-02-06T17:24:00.947+0000] {subprocess.py:93} INFO - 	at org.apache.spark.sql.catalyst.trees.TreeNode.transformDownWithPruning(TreeNode.scala:461) [ex02-ingestion-assembly.jar:1.0.0]
[2026-02-06T17:24:00.947+0000] {subprocess.py:93} INFO - 	at org.apache.spark.sql.catalyst.plans.logical.LogicalPlan.org$apache$spark$sql$catalyst$plans$logical$AnalysisHelper$$super$transformDownWithPruning(LogicalPlan.scala:32) [ex02-ingestion-assembly.jar:1.0.0]
[2026-02-06T17:24:00.947+0000] {subprocess.py:93} INFO - 	at org.apache.spark.sql.catalyst.plans.logical.AnalysisHelper.transformDownWithPruning(AnalysisHelper.scala:267) [ex02-ingestion-assembly.jar:1.0.0]
[2026-02-06T17:24:00.947+0000] {subprocess.py:93} INFO - 	at org.apache.spark.sql.catalyst.plans.logical.AnalysisHelper.transformDownWithPruning$(AnalysisHelper.scala:263) [ex02-ingestion-assembly.jar:1.0.0]
[2026-02-06T17:24:00.948+0000] {subprocess.py:93} INFO - 	at org.apache.spark.sql.catalyst.plans.logical.LogicalPlan.transformDownWithPruning(LogicalPlan.scala:32) [ex02-ingestion-assembly.jar:1.0.0]
[2026-02-06T17:24:00.948+0000] {subprocess.py:93} INFO - 	at org.apache.spark.sql.catalyst.plans.logical.LogicalPlan.transformDownWithPruning(LogicalPlan.scala:32) [ex02-ingestion-assembly.jar:1.0.0]
[2026-02-06T17:24:00.948+0000] {subprocess.py:93} INFO - 	at org.apache.spark.sql.catalyst.trees.TreeNode.transformDown(TreeNode.scala:437) [ex02-ingestion-assembly.jar:1.0.0]
[2026-02-06T17:24:00.948+0000] {subprocess.py:93} INFO - 	at org.apache.spark.sql.execution.QueryExecution.eagerlyExecuteCommands(QueryExecution.scala:98) [ex02-ingestion-assembly.jar:1.0.0]
[2026-02-06T17:24:00.948+0000] {subprocess.py:93} INFO - 	at org.apache.spark.sql.execution.QueryExecution.commandExecuted$lzycompute(QueryExecution.scala:85) [ex02-ingestion-assembly.jar:1.0.0]
[2026-02-06T17:24:00.948+0000] {subprocess.py:93} INFO - 	at org.apache.spark.sql.execution.QueryExecution.commandExecuted(QueryExecution.scala:83) [ex02-ingestion-assembly.jar:1.0.0]
[2026-02-06T17:24:00.948+0000] {subprocess.py:93} INFO - 	at org.apache.spark.sql.execution.QueryExecution.assertCommandExecuted(QueryExecution.scala:142) [ex02-ingestion-assembly.jar:1.0.0]
[2026-02-06T17:24:00.948+0000] {subprocess.py:93} INFO - 	at org.apache.spark.sql.DataFrameWriter.runCommand(DataFrameWriter.scala:869) [ex02-ingestion-assembly.jar:1.0.0]
[2026-02-06T17:24:00.949+0000] {subprocess.py:93} INFO - 	at org.apache.spark.sql.DataFrameWriter.saveToV1Source(DataFrameWriter.scala:391) [ex02-ingestion-assembly.jar:1.0.0]
[2026-02-06T17:24:00.949+0000] {subprocess.py:93} INFO - 	at org.apache.spark.sql.DataFrameWriter.saveInternal(DataFrameWriter.scala:364) [ex02-ingestion-assembly.jar:1.0.0]
[2026-02-06T17:24:00.949+0000] {subprocess.py:93} INFO - 	at org.apache.spark.sql.DataFrameWriter.save(DataFrameWriter.scala:243) [ex02-ingestion-assembly.jar:1.0.0]
[2026-02-06T17:24:00.949+0000] {subprocess.py:93} INFO - 	at org.apache.spark.sql.DataFrameWriter.parquet(DataFrameWriter.scala:802) [ex02-ingestion-assembly.jar:1.0.0]
[2026-02-06T17:24:00.949+0000] {subprocess.py:93} INFO - 	at fr.cytech.ingestion.Main$.main(Main.scala:107) [ex02-ingestion-assembly.jar:1.0.0]
[2026-02-06T17:24:00.949+0000] {subprocess.py:93} INFO - 	at fr.cytech.ingestion.Main.main(Main.scala) [ex02-ingestion-assembly.jar:1.0.0]
[2026-02-06T17:24:00.949+0000] {subprocess.py:93} INFO - Caused by: org.apache.hadoop.fs.UnsupportedFileSystemException: No FileSystem for scheme "file"
[2026-02-06T17:24:00.949+0000] {subprocess.py:93} INFO - 	at org.apache.hadoop.fs.FileSystem.getFileSystemClass(FileSystem.java:3443) ~[ex02-ingestion-assembly.jar:1.0.0]
[2026-02-06T17:24:00.949+0000] {subprocess.py:93} INFO - 	at org.apache.hadoop.fs.FileSystem.createFileSystem(FileSystem.java:3466) ~[ex02-ingestion-assembly.jar:1.0.0]
[2026-02-06T17:24:00.949+0000] {subprocess.py:93} INFO - 	at org.apache.hadoop.fs.FileSystem.access$300(FileSystem.java:174) ~[ex02-ingestion-assembly.jar:1.0.0]
[2026-02-06T17:24:00.949+0000] {subprocess.py:93} INFO - 	at org.apache.hadoop.fs.FileSystem$Cache.getInternal(FileSystem.java:3574) ~[ex02-ingestion-assembly.jar:1.0.0]
[2026-02-06T17:24:00.949+0000] {subprocess.py:93} INFO - 	at org.apache.hadoop.fs.FileSystem$Cache.get(FileSystem.java:3521) ~[ex02-ingestion-assembly.jar:1.0.0]
[2026-02-06T17:24:00.949+0000] {subprocess.py:93} INFO - 	at org.apache.hadoop.fs.FileSystem.get(FileSystem.java:540) ~[ex02-ingestion-assembly.jar:1.0.0]
[2026-02-06T17:24:00.949+0000] {subprocess.py:93} INFO - 	at org.apache.hadoop.fs.FileSystem.getLocal(FileSystem.java:496) ~[ex02-ingestion-assembly.jar:1.0.0]
[2026-02-06T17:24:00.950+0000] {subprocess.py:93} INFO - 	at org.apache.hadoop.fs.LocalDirAllocator$AllocatorPerContext.confChanged(LocalDirAllocator.java:316) ~[ex02-ingestion-assembly.jar:1.0.0]
[2026-02-06T17:24:00.950+0000] {subprocess.py:93} INFO - 	at org.apache.hadoop.fs.LocalDirAllocator$AllocatorPerContext.getLocalPathForWrite(LocalDirAllocator.java:393) ~[ex02-ingestion-assembly.jar:1.0.0]
[2026-02-06T17:24:00.950+0000] {subprocess.py:93} INFO - 	at org.apache.hadoop.fs.LocalDirAllocator.getLocalPathForWrite(LocalDirAllocator.java:165) ~[ex02-ingestion-assembly.jar:1.0.0]
[2026-02-06T17:24:00.950+0000] {subprocess.py:93} INFO - 	at org.apache.hadoop.fs.LocalDirAllocator.getLocalPathForWrite(LocalDirAllocator.java:146) ~[ex02-ingestion-assembly.jar:1.0.0]
[2026-02-06T17:24:00.950+0000] {subprocess.py:93} INFO - 	at org.apache.hadoop.fs.s3a.S3AFileSystem.createTmpFileForWrite(S3AFileSystem.java:1282) ~[ex02-ingestion-assembly.jar:1.0.0]
[2026-02-06T17:24:00.950+0000] {subprocess.py:93} INFO - 	at org.apache.hadoop.fs.s3a.S3ADataBlocks$DiskBlockFactory.create(S3ADataBlocks.java:816) ~[ex02-ingestion-assembly.jar:1.0.0]
[2026-02-06T17:24:00.950+0000] {subprocess.py:93} INFO - 	at org.apache.hadoop.fs.s3a.S3ABlockOutputStream.createBlockIfNeeded(S3ABlockOutputStream.java:211) ~[ex02-ingestion-assembly.jar:1.0.0]
[2026-02-06T17:24:00.950+0000] {subprocess.py:93} INFO - 	at org.apache.hadoop.fs.s3a.S3ABlockOutputStream.<init>(S3ABlockOutputStream.java:188) ~[ex02-ingestion-assembly.jar:1.0.0]
[2026-02-06T17:24:00.951+0000] {subprocess.py:93} INFO - 	at org.apache.hadoop.fs.s3a.S3AFileSystem.innerCreateFile(S3AFileSystem.java:1727) ~[ex02-ingestion-assembly.jar:1.0.0]
[2026-02-06T17:24:00.951+0000] {subprocess.py:93} INFO - 	at org.apache.hadoop.fs.s3a.S3AFileSystem.lambda$create$6(S3AFileSystem.java:1646) ~[ex02-ingestion-assembly.jar:1.0.0]
[2026-02-06T17:24:00.951+0000] {subprocess.py:93} INFO - 	at org.apache.hadoop.fs.statistics.impl.IOStatisticsBinding.lambda$trackDurationOfOperation$5(IOStatisticsBinding.java:499) ~[ex02-ingestion-assembly.jar:1.0.0]
[2026-02-06T17:24:00.951+0000] {subprocess.py:93} INFO - 	at org.apache.hadoop.fs.statistics.impl.IOStatisticsBinding.trackDuration(IOStatisticsBinding.java:444) ~[ex02-ingestion-assembly.jar:1.0.0]
[2026-02-06T17:24:00.951+0000] {subprocess.py:93} INFO - 	at org.apache.hadoop.fs.s3a.S3AFileSystem.trackDurationAndSpan(S3AFileSystem.java:2337) ~[ex02-ingestion-assembly.jar:1.0.0]
[2026-02-06T17:24:00.952+0000] {subprocess.py:93} INFO - 	at org.apache.hadoop.fs.s3a.S3AFileSystem.trackDurationAndSpan(S3AFileSystem.java:2356) ~[ex02-ingestion-assembly.jar:1.0.0]
[2026-02-06T17:24:00.952+0000] {subprocess.py:93} INFO - 	at org.apache.hadoop.fs.s3a.S3AFileSystem.create(S3AFileSystem.java:1645) ~[ex02-ingestion-assembly.jar:1.0.0]
[2026-02-06T17:24:00.952+0000] {subprocess.py:93} INFO - 	at org.apache.hadoop.fs.FileSystem.create(FileSystem.java:1195) ~[ex02-ingestion-assembly.jar:1.0.0]
[2026-02-06T17:24:00.952+0000] {subprocess.py:93} INFO - 	at org.apache.hadoop.fs.FileSystem.create(FileSystem.java:1175) ~[ex02-ingestion-assembly.jar:1.0.0]
[2026-02-06T17:24:00.952+0000] {subprocess.py:93} INFO - 	at org.apache.parquet.hadoop.util.HadoopOutputFile.create(HadoopOutputFile.java:74) ~[ex02-ingestion-assembly.jar:1.0.0]
[2026-02-06T17:24:00.952+0000] {subprocess.py:93} INFO - 	at org.apache.parquet.hadoop.ParquetFileWriter.<init>(ParquetFileWriter.java:347) ~[ex02-ingestion-assembly.jar:1.0.0]
[2026-02-06T17:24:00.952+0000] {subprocess.py:93} INFO - 	at org.apache.parquet.hadoop.ParquetFileWriter.<init>(ParquetFileWriter.java:314) ~[ex02-ingestion-assembly.jar:1.0.0]
[2026-02-06T17:24:00.952+0000] {subprocess.py:93} INFO - 	at org.apache.parquet.hadoop.ParquetOutputFormat.getRecordWriter(ParquetOutputFormat.java:484) ~[ex02-ingestion-assembly.jar:1.0.0]
[2026-02-06T17:24:00.952+0000] {subprocess.py:93} INFO - 	at org.apache.parquet.hadoop.ParquetOutputFormat.getRecordWriter(ParquetOutputFormat.java:422) ~[ex02-ingestion-assembly.jar:1.0.0]
[2026-02-06T17:24:00.952+0000] {subprocess.py:93} INFO - 	at org.apache.parquet.hadoop.ParquetOutputFormat.getRecordWriter(ParquetOutputFormat.java:411) ~[ex02-ingestion-assembly.jar:1.0.0]
[2026-02-06T17:24:00.952+0000] {subprocess.py:93} INFO - 	at org.apache.spark.sql.execution.datasources.parquet.ParquetOutputWriter.<init>(ParquetOutputWriter.scala:36) ~[ex02-ingestion-assembly.jar:1.0.0]
[2026-02-06T17:24:00.953+0000] {subprocess.py:93} INFO - 	at org.apache.spark.sql.execution.datasources.parquet.ParquetUtils$$anon$1.newInstance(ParquetUtils.scala:490) ~[ex02-ingestion-assembly.jar:1.0.0]
[2026-02-06T17:24:00.953+0000] {subprocess.py:93} INFO - 	at org.apache.spark.sql.execution.datasources.SingleDirectoryDataWriter.newOutputWriter(FileFormatDataWriter.scala:161) ~[ex02-ingestion-assembly.jar:1.0.0]
[2026-02-06T17:24:00.953+0000] {subprocess.py:93} INFO - 	at org.apache.spark.sql.execution.datasources.SingleDirectoryDataWriter.<init>(FileFormatDataWriter.scala:146) ~[ex02-ingestion-assembly.jar:1.0.0]
[2026-02-06T17:24:00.953+0000] {subprocess.py:93} INFO - 	at org.apache.spark.sql.execution.datasources.FileFormatWriter$.executeTask(FileFormatWriter.scala:389) ~[ex02-ingestion-assembly.jar:1.0.0]
[2026-02-06T17:24:00.953+0000] {subprocess.py:93} INFO - 	at org.apache.spark.sql.execution.datasources.WriteFilesExec.$anonfun$doExecuteWrite$1(WriteFiles.scala:100) ~[ex02-ingestion-assembly.jar:1.0.0]
[2026-02-06T17:24:00.953+0000] {subprocess.py:93} INFO - 	at org.apache.spark.rdd.RDD.$anonfun$mapPartitionsInternal$2(RDD.scala:893) ~[ex02-ingestion-assembly.jar:1.0.0]
[2026-02-06T17:24:00.953+0000] {subprocess.py:93} INFO - 	at org.apache.spark.rdd.RDD.$anonfun$mapPartitionsInternal$2$adapted(RDD.scala:893) ~[ex02-ingestion-assembly.jar:1.0.0]
[2026-02-06T17:24:00.953+0000] {subprocess.py:93} INFO - 	at org.apache.spark.rdd.MapPartitionsRDD.compute(MapPartitionsRDD.scala:52) ~[ex02-ingestion-assembly.jar:1.0.0]
[2026-02-06T17:24:00.953+0000] {subprocess.py:93} INFO - 	at org.apache.spark.rdd.RDD.computeOrReadCheckpoint(RDD.scala:367) ~[ex02-ingestion-assembly.jar:1.0.0]
[2026-02-06T17:24:00.954+0000] {subprocess.py:93} INFO - 	at org.apache.spark.rdd.RDD.iterator(RDD.scala:331) ~[ex02-ingestion-assembly.jar:1.0.0]
[2026-02-06T17:24:00.954+0000] {subprocess.py:93} INFO - 	at org.apache.spark.scheduler.ResultTask.runTask(ResultTask.scala:93) ~[ex02-ingestion-assembly.jar:1.0.0]
[2026-02-06T17:24:00.954+0000] {subprocess.py:93} INFO - 	at org.apache.spark.TaskContext.runTaskWithListeners(TaskContext.scala:166) ~[ex02-ingestion-assembly.jar:1.0.0]
[2026-02-06T17:24:00.954+0000] {subprocess.py:93} INFO - 	at org.apache.spark.scheduler.Task.run(Task.scala:141) ~[ex02-ingestion-assembly.jar:1.0.0]
[2026-02-06T17:24:00.954+0000] {subprocess.py:93} INFO - 	at org.apache.spark.executor.Executor$TaskRunner.$anonfun$run$4(Executor.scala:620) ~[ex02-ingestion-assembly.jar:1.0.0]
[2026-02-06T17:24:00.954+0000] {subprocess.py:93} INFO - 	at org.apache.spark.util.SparkErrorUtils.tryWithSafeFinally(SparkErrorUtils.scala:64) ~[ex02-ingestion-assembly.jar:1.0.0]
[2026-02-06T17:24:00.954+0000] {subprocess.py:93} INFO - 	at org.apache.spark.util.SparkErrorUtils.tryWithSafeFinally$(SparkErrorUtils.scala:61) ~[ex02-ingestion-assembly.jar:1.0.0]
[2026-02-06T17:24:00.954+0000] {subprocess.py:93} INFO - 	at org.apache.spark.util.Utils$.tryWithSafeFinally(Utils.scala:94) ~[ex02-ingestion-assembly.jar:1.0.0]
[2026-02-06T17:24:00.954+0000] {subprocess.py:93} INFO - 	at org.apache.spark.executor.Executor$TaskRunner.run(Executor.scala:623) ~[ex02-ingestion-assembly.jar:1.0.0]
[2026-02-06T17:24:00.954+0000] {subprocess.py:93} INFO - 	at java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1136) ~[?:?]
[2026-02-06T17:24:00.954+0000] {subprocess.py:93} INFO - 	at java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:635) ~[?:?]
[2026-02-06T17:24:00.954+0000] {subprocess.py:93} INFO - 	at java.lang.Thread.run(Thread.java:840) ~[?:?]
[2026-02-06T17:24:01.065+0000] {subprocess.py:93} INFO - Exception in thread "main" org.apache.spark.SparkException: Job aborted due to stage failure: Task 0 in stage 1.0 failed 1 times, most recent failure: Lost task 0.0 in stage 1.0 (TID 1) (4d21249f976f executor driver): org.apache.hadoop.fs.UnsupportedFileSystemException: No FileSystem for scheme "file"
[2026-02-06T17:24:01.066+0000] {subprocess.py:93} INFO - 	at org.apache.hadoop.fs.FileSystem.getFileSystemClass(FileSystem.java:3443)
[2026-02-06T17:24:01.066+0000] {subprocess.py:93} INFO - 	at org.apache.hadoop.fs.FileSystem.createFileSystem(FileSystem.java:3466)
[2026-02-06T17:24:01.066+0000] {subprocess.py:93} INFO - 	at org.apache.hadoop.fs.FileSystem.access$300(FileSystem.java:174)
[2026-02-06T17:24:01.066+0000] {subprocess.py:93} INFO - 	at org.apache.hadoop.fs.FileSystem$Cache.getInternal(FileSystem.java:3574)
[2026-02-06T17:24:01.067+0000] {subprocess.py:93} INFO - 	at org.apache.hadoop.fs.FileSystem$Cache.get(FileSystem.java:3521)
[2026-02-06T17:24:01.067+0000] {subprocess.py:93} INFO - 	at org.apache.hadoop.fs.FileSystem.get(FileSystem.java:540)
[2026-02-06T17:24:01.067+0000] {subprocess.py:93} INFO - 	at org.apache.hadoop.fs.FileSystem.getLocal(FileSystem.java:496)
[2026-02-06T17:24:01.067+0000] {subprocess.py:93} INFO - 	at org.apache.hadoop.fs.LocalDirAllocator$AllocatorPerContext.confChanged(LocalDirAllocator.java:316)
[2026-02-06T17:24:01.067+0000] {subprocess.py:93} INFO - 	at org.apache.hadoop.fs.LocalDirAllocator$AllocatorPerContext.getLocalPathForWrite(LocalDirAllocator.java:393)
[2026-02-06T17:24:01.068+0000] {subprocess.py:93} INFO - 	at org.apache.hadoop.fs.LocalDirAllocator.getLocalPathForWrite(LocalDirAllocator.java:165)
[2026-02-06T17:24:01.068+0000] {subprocess.py:93} INFO - 	at org.apache.hadoop.fs.LocalDirAllocator.getLocalPathForWrite(LocalDirAllocator.java:146)
[2026-02-06T17:24:01.068+0000] {subprocess.py:93} INFO - 	at org.apache.hadoop.fs.s3a.S3AFileSystem.createTmpFileForWrite(S3AFileSystem.java:1282)
[2026-02-06T17:24:01.068+0000] {subprocess.py:93} INFO - 	at org.apache.hadoop.fs.s3a.S3ADataBlocks$DiskBlockFactory.create(S3ADataBlocks.java:816)
[2026-02-06T17:24:01.068+0000] {subprocess.py:93} INFO - 	at org.apache.hadoop.fs.s3a.S3ABlockOutputStream.createBlockIfNeeded(S3ABlockOutputStream.java:211)
[2026-02-06T17:24:01.068+0000] {subprocess.py:93} INFO - 	at org.apache.hadoop.fs.s3a.S3ABlockOutputStream.<init>(S3ABlockOutputStream.java:188)
[2026-02-06T17:24:01.068+0000] {subprocess.py:93} INFO - 	at org.apache.hadoop.fs.s3a.S3AFileSystem.innerCreateFile(S3AFileSystem.java:1727)
[2026-02-06T17:24:01.068+0000] {subprocess.py:93} INFO - 	at org.apache.hadoop.fs.s3a.S3AFileSystem.lambda$create$6(S3AFileSystem.java:1646)
[2026-02-06T17:24:01.068+0000] {subprocess.py:93} INFO - 	at org.apache.hadoop.fs.statistics.impl.IOStatisticsBinding.lambda$trackDurationOfOperation$5(IOStatisticsBinding.java:499)
[2026-02-06T17:24:01.068+0000] {subprocess.py:93} INFO - 	at org.apache.hadoop.fs.statistics.impl.IOStatisticsBinding.trackDuration(IOStatisticsBinding.java:444)
[2026-02-06T17:24:01.068+0000] {subprocess.py:93} INFO - 	at org.apache.hadoop.fs.s3a.S3AFileSystem.trackDurationAndSpan(S3AFileSystem.java:2337)
[2026-02-06T17:24:01.069+0000] {subprocess.py:93} INFO - 	at org.apache.hadoop.fs.s3a.S3AFileSystem.trackDurationAndSpan(S3AFileSystem.java:2356)
[2026-02-06T17:24:01.069+0000] {subprocess.py:93} INFO - 	at org.apache.hadoop.fs.s3a.S3AFileSystem.create(S3AFileSystem.java:1645)
[2026-02-06T17:24:01.069+0000] {subprocess.py:93} INFO - 	at org.apache.hadoop.fs.FileSystem.create(FileSystem.java:1195)
[2026-02-06T17:24:01.069+0000] {subprocess.py:93} INFO - 	at org.apache.hadoop.fs.FileSystem.create(FileSystem.java:1175)
[2026-02-06T17:24:01.069+0000] {subprocess.py:93} INFO - 	at org.apache.parquet.hadoop.util.HadoopOutputFile.create(HadoopOutputFile.java:74)
[2026-02-06T17:24:01.069+0000] {subprocess.py:93} INFO - 	at org.apache.parquet.hadoop.ParquetFileWriter.<init>(ParquetFileWriter.java:347)
[2026-02-06T17:24:01.069+0000] {subprocess.py:93} INFO - 	at org.apache.parquet.hadoop.ParquetFileWriter.<init>(ParquetFileWriter.java:314)
[2026-02-06T17:24:01.069+0000] {subprocess.py:93} INFO - 	at org.apache.parquet.hadoop.ParquetOutputFormat.getRecordWriter(ParquetOutputFormat.java:484)
[2026-02-06T17:24:01.069+0000] {subprocess.py:93} INFO - 	at org.apache.parquet.hadoop.ParquetOutputFormat.getRecordWriter(ParquetOutputFormat.java:422)
[2026-02-06T17:24:01.070+0000] {subprocess.py:93} INFO - 	at org.apache.parquet.hadoop.ParquetOutputFormat.getRecordWriter(ParquetOutputFormat.java:411)
[2026-02-06T17:24:01.070+0000] {subprocess.py:93} INFO - 	at org.apache.spark.sql.execution.datasources.parquet.ParquetOutputWriter.<init>(ParquetOutputWriter.scala:36)
[2026-02-06T17:24:01.070+0000] {subprocess.py:93} INFO - 	at org.apache.spark.sql.execution.datasources.parquet.ParquetUtils$$anon$1.newInstance(ParquetUtils.scala:490)
[2026-02-06T17:24:01.070+0000] {subprocess.py:93} INFO - 	at org.apache.spark.sql.execution.datasources.SingleDirectoryDataWriter.newOutputWriter(FileFormatDataWriter.scala:161)
[2026-02-06T17:24:01.070+0000] {subprocess.py:93} INFO - 	at org.apache.spark.sql.execution.datasources.SingleDirectoryDataWriter.<init>(FileFormatDataWriter.scala:146)
[2026-02-06T17:24:01.070+0000] {subprocess.py:93} INFO - 	at org.apache.spark.sql.execution.datasources.FileFormatWriter$.executeTask(FileFormatWriter.scala:389)
[2026-02-06T17:24:01.070+0000] {subprocess.py:93} INFO - 	at org.apache.spark.sql.execution.datasources.WriteFilesExec.$anonfun$doExecuteWrite$1(WriteFiles.scala:100)
[2026-02-06T17:24:01.070+0000] {subprocess.py:93} INFO - 	at org.apache.spark.rdd.RDD.$anonfun$mapPartitionsInternal$2(RDD.scala:893)
[2026-02-06T17:24:01.070+0000] {subprocess.py:93} INFO - 	at org.apache.spark.rdd.RDD.$anonfun$mapPartitionsInternal$2$adapted(RDD.scala:893)
[2026-02-06T17:24:01.070+0000] {subprocess.py:93} INFO - 	at org.apache.spark.rdd.MapPartitionsRDD.compute(MapPartitionsRDD.scala:52)
[2026-02-06T17:24:01.071+0000] {subprocess.py:93} INFO - 	at org.apache.spark.rdd.RDD.computeOrReadCheckpoint(RDD.scala:367)
[2026-02-06T17:24:01.071+0000] {subprocess.py:93} INFO - 	at org.apache.spark.rdd.RDD.iterator(RDD.scala:331)
[2026-02-06T17:24:01.071+0000] {subprocess.py:93} INFO - 	at org.apache.spark.scheduler.ResultTask.runTask(ResultTask.scala:93)
[2026-02-06T17:24:01.071+0000] {subprocess.py:93} INFO - 	at org.apache.spark.TaskContext.runTaskWithListeners(TaskContext.scala:166)
[2026-02-06T17:24:01.071+0000] {subprocess.py:93} INFO - 	at org.apache.spark.scheduler.Task.run(Task.scala:141)
[2026-02-06T17:24:01.071+0000] {subprocess.py:93} INFO - 	at org.apache.spark.executor.Executor$TaskRunner.$anonfun$run$4(Executor.scala:620)
[2026-02-06T17:24:01.071+0000] {subprocess.py:93} INFO - 	at org.apache.spark.util.SparkErrorUtils.tryWithSafeFinally(SparkErrorUtils.scala:64)
[2026-02-06T17:24:01.071+0000] {subprocess.py:93} INFO - 	at org.apache.spark.util.SparkErrorUtils.tryWithSafeFinally$(SparkErrorUtils.scala:61)
[2026-02-06T17:24:01.071+0000] {subprocess.py:93} INFO - 	at org.apache.spark.util.Utils$.tryWithSafeFinally(Utils.scala:94)
[2026-02-06T17:24:01.071+0000] {subprocess.py:93} INFO - 	at org.apache.spark.executor.Executor$TaskRunner.run(Executor.scala:623)
[2026-02-06T17:24:01.071+0000] {subprocess.py:93} INFO - 	at java.base/java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1136)
[2026-02-06T17:24:01.071+0000] {subprocess.py:93} INFO - 	at java.base/java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:635)
[2026-02-06T17:24:01.072+0000] {subprocess.py:93} INFO - 	at java.base/java.lang.Thread.run(Thread.java:840)
[2026-02-06T17:24:01.072+0000] {subprocess.py:93} INFO - 
[2026-02-06T17:24:01.072+0000] {subprocess.py:93} INFO - Driver stacktrace:
[2026-02-06T17:24:01.072+0000] {subprocess.py:93} INFO - 	at org.apache.spark.scheduler.DAGScheduler.failJobAndIndependentStages(DAGScheduler.scala:2856)
[2026-02-06T17:24:01.072+0000] {subprocess.py:93} INFO - 	at org.apache.spark.scheduler.DAGScheduler.$anonfun$abortStage$2(DAGScheduler.scala:2792)
[2026-02-06T17:24:01.072+0000] {subprocess.py:93} INFO - 	at org.apache.spark.scheduler.DAGScheduler.$anonfun$abortStage$2$adapted(DAGScheduler.scala:2791)
[2026-02-06T17:24:01.072+0000] {subprocess.py:93} INFO - 	at scala.collection.immutable.List.foreach(List.scala:323)
[2026-02-06T17:24:01.072+0000] {subprocess.py:93} INFO - 	at org.apache.spark.scheduler.DAGScheduler.abortStage(DAGScheduler.scala:2791)
[2026-02-06T17:24:01.072+0000] {subprocess.py:93} INFO - 	at org.apache.spark.scheduler.DAGScheduler.$anonfun$handleTaskSetFailed$1(DAGScheduler.scala:1247)
[2026-02-06T17:24:01.072+0000] {subprocess.py:93} INFO - 	at org.apache.spark.scheduler.DAGScheduler.$anonfun$handleTaskSetFailed$1$adapted(DAGScheduler.scala:1247)
[2026-02-06T17:24:01.072+0000] {subprocess.py:93} INFO - 	at scala.Option.foreach(Option.scala:437)
[2026-02-06T17:24:01.072+0000] {subprocess.py:93} INFO - 	at org.apache.spark.scheduler.DAGScheduler.handleTaskSetFailed(DAGScheduler.scala:1247)
[2026-02-06T17:24:01.073+0000] {subprocess.py:93} INFO - 	at org.apache.spark.scheduler.DAGSchedulerEventProcessLoop.doOnReceive(DAGScheduler.scala:3060)
[2026-02-06T17:24:01.073+0000] {subprocess.py:93} INFO - 	at org.apache.spark.scheduler.DAGSchedulerEventProcessLoop.onReceive(DAGScheduler.scala:2994)
[2026-02-06T17:24:01.073+0000] {subprocess.py:93} INFO - 	at org.apache.spark.scheduler.DAGSchedulerEventProcessLoop.onReceive(DAGScheduler.scala:2983)
[2026-02-06T17:24:01.073+0000] {subprocess.py:93} INFO - 	at org.apache.spark.util.EventLoop$$anon$1.run(EventLoop.scala:49)
[2026-02-06T17:24:01.073+0000] {subprocess.py:93} INFO - 	at org.apache.spark.scheduler.DAGScheduler.runJob(DAGScheduler.scala:989)
[2026-02-06T17:24:01.073+0000] {subprocess.py:93} INFO - 	at org.apache.spark.SparkContext.runJob(SparkContext.scala:2393)
[2026-02-06T17:24:01.073+0000] {subprocess.py:93} INFO - 	at org.apache.spark.sql.execution.datasources.FileFormatWriter$.$anonfun$executeWrite$4(FileFormatWriter.scala:307)
[2026-02-06T17:24:01.073+0000] {subprocess.py:93} INFO - 	at org.apache.spark.sql.execution.datasources.FileFormatWriter$.writeAndCommit(FileFormatWriter.scala:271)
[2026-02-06T17:24:01.073+0000] {subprocess.py:93} INFO - 	at org.apache.spark.sql.execution.datasources.FileFormatWriter$.executeWrite(FileFormatWriter.scala:304)
[2026-02-06T17:24:01.073+0000] {subprocess.py:93} INFO - 	at org.apache.spark.sql.execution.datasources.FileFormatWriter$.write(FileFormatWriter.scala:190)
[2026-02-06T17:24:01.073+0000] {subprocess.py:93} INFO - 	at org.apache.spark.sql.execution.datasources.InsertIntoHadoopFsRelationCommand.run(InsertIntoHadoopFsRelationCommand.scala:190)
[2026-02-06T17:24:01.073+0000] {subprocess.py:93} INFO - 	at org.apache.spark.sql.execution.command.DataWritingCommandExec.sideEffectResult$lzycompute(commands.scala:113)
[2026-02-06T17:24:01.073+0000] {subprocess.py:93} INFO - 	at org.apache.spark.sql.execution.command.DataWritingCommandExec.sideEffectResult(commands.scala:111)
[2026-02-06T17:24:01.073+0000] {subprocess.py:93} INFO - 	at org.apache.spark.sql.execution.command.DataWritingCommandExec.executeCollect(commands.scala:125)
[2026-02-06T17:24:01.074+0000] {subprocess.py:93} INFO - 	at org.apache.spark.sql.execution.QueryExecution$$anonfun$eagerlyExecuteCommands$1.$anonfun$applyOrElse$1(QueryExecution.scala:107)
[2026-02-06T17:24:01.074+0000] {subprocess.py:93} INFO - 	at org.apache.spark.sql.execution.SQLExecution$.$anonfun$withNewExecutionId$6(SQLExecution.scala:125)
[2026-02-06T17:24:01.074+0000] {subprocess.py:93} INFO - 	at org.apache.spark.sql.execution.SQLExecution$.withSQLConfPropagated(SQLExecution.scala:201)
[2026-02-06T17:24:01.074+0000] {subprocess.py:93} INFO - 	at org.apache.spark.sql.execution.SQLExecution$.$anonfun$withNewExecutionId$1(SQLExecution.scala:108)
[2026-02-06T17:24:01.074+0000] {subprocess.py:93} INFO - 	at org.apache.spark.sql.SparkSession.withActive(SparkSession.scala:900)
[2026-02-06T17:24:01.074+0000] {subprocess.py:93} INFO - 	at org.apache.spark.sql.execution.SQLExecution$.withNewExecutionId(SQLExecution.scala:66)
[2026-02-06T17:24:01.074+0000] {subprocess.py:93} INFO - 	at org.apache.spark.sql.execution.QueryExecution$$anonfun$eagerlyExecuteCommands$1.applyOrElse(QueryExecution.scala:107)
[2026-02-06T17:24:01.074+0000] {subprocess.py:93} INFO - 	at org.apache.spark.sql.execution.QueryExecution$$anonfun$eagerlyExecuteCommands$1.applyOrElse(QueryExecution.scala:98)
[2026-02-06T17:24:01.074+0000] {subprocess.py:93} INFO - 	at org.apache.spark.sql.catalyst.trees.TreeNode.$anonfun$transformDownWithPruning$1(TreeNode.scala:461)
[2026-02-06T17:24:01.074+0000] {subprocess.py:93} INFO - 	at org.apache.spark.sql.catalyst.trees.CurrentOrigin$.withOrigin(origin.scala:76)
[2026-02-06T17:24:01.074+0000] {subprocess.py:93} INFO - 	at org.apache.spark.sql.catalyst.trees.TreeNode.transformDownWithPruning(TreeNode.scala:461)
[2026-02-06T17:24:01.074+0000] {subprocess.py:93} INFO - 	at org.apache.spark.sql.catalyst.plans.logical.LogicalPlan.org$apache$spark$sql$catalyst$plans$logical$AnalysisHelper$$super$transformDownWithPruning(LogicalPlan.scala:32)
[2026-02-06T17:24:01.074+0000] {subprocess.py:93} INFO - 	at org.apache.spark.sql.catalyst.plans.logical.AnalysisHelper.transformDownWithPruning(AnalysisHelper.scala:267)
[2026-02-06T17:24:01.074+0000] {subprocess.py:93} INFO - 	at org.apache.spark.sql.catalyst.plans.logical.AnalysisHelper.transformDownWithPruning$(AnalysisHelper.scala:263)
[2026-02-06T17:24:01.074+0000] {subprocess.py:93} INFO - 	at org.apache.spark.sql.catalyst.plans.logical.LogicalPlan.transformDownWithPruning(LogicalPlan.scala:32)
[2026-02-06T17:24:01.074+0000] {subprocess.py:93} INFO - 	at org.apache.spark.sql.catalyst.plans.logical.LogicalPlan.transformDownWithPruning(LogicalPlan.scala:32)
[2026-02-06T17:24:01.074+0000] {subprocess.py:93} INFO - 	at org.apache.spark.sql.catalyst.trees.TreeNode.transformDown(TreeNode.scala:437)
[2026-02-06T17:24:01.074+0000] {subprocess.py:93} INFO - 	at org.apache.spark.sql.execution.QueryExecution.eagerlyExecuteCommands(QueryExecution.scala:98)
[2026-02-06T17:24:01.075+0000] {subprocess.py:93} INFO - 	at org.apache.spark.sql.execution.QueryExecution.commandExecuted$lzycompute(QueryExecution.scala:85)
[2026-02-06T17:24:01.075+0000] {subprocess.py:93} INFO - 	at org.apache.spark.sql.execution.QueryExecution.commandExecuted(QueryExecution.scala:83)
[2026-02-06T17:24:01.075+0000] {subprocess.py:93} INFO - 	at org.apache.spark.sql.execution.QueryExecution.assertCommandExecuted(QueryExecution.scala:142)
[2026-02-06T17:24:01.075+0000] {subprocess.py:93} INFO - 	at org.apache.spark.sql.DataFrameWriter.runCommand(DataFrameWriter.scala:869)
[2026-02-06T17:24:01.075+0000] {subprocess.py:93} INFO - 	at org.apache.spark.sql.DataFrameWriter.saveToV1Source(DataFrameWriter.scala:391)
[2026-02-06T17:24:01.075+0000] {subprocess.py:93} INFO - 	at org.apache.spark.sql.DataFrameWriter.saveInternal(DataFrameWriter.scala:364)
[2026-02-06T17:24:01.075+0000] {subprocess.py:93} INFO - 	at org.apache.spark.sql.DataFrameWriter.save(DataFrameWriter.scala:243)
[2026-02-06T17:24:01.076+0000] {subprocess.py:93} INFO - 	at org.apache.spark.sql.DataFrameWriter.parquet(DataFrameWriter.scala:802)
[2026-02-06T17:24:01.076+0000] {subprocess.py:93} INFO - 	at fr.cytech.ingestion.Main$.main(Main.scala:107)
[2026-02-06T17:24:01.077+0000] {subprocess.py:93} INFO - 	at fr.cytech.ingestion.Main.main(Main.scala)
[2026-02-06T17:24:01.077+0000] {subprocess.py:93} INFO - Caused by: org.apache.hadoop.fs.UnsupportedFileSystemException: No FileSystem for scheme "file"
[2026-02-06T17:24:01.077+0000] {subprocess.py:93} INFO - 	at org.apache.hadoop.fs.FileSystem.getFileSystemClass(FileSystem.java:3443)
[2026-02-06T17:24:01.078+0000] {subprocess.py:93} INFO - 	at org.apache.hadoop.fs.FileSystem.createFileSystem(FileSystem.java:3466)
[2026-02-06T17:24:01.078+0000] {subprocess.py:93} INFO - 	at org.apache.hadoop.fs.FileSystem.access$300(FileSystem.java:174)
[2026-02-06T17:24:01.078+0000] {subprocess.py:93} INFO - 	at org.apache.hadoop.fs.FileSystem$Cache.getInternal(FileSystem.java:3574)
[2026-02-06T17:24:01.078+0000] {subprocess.py:93} INFO - 	at org.apache.hadoop.fs.FileSystem$Cache.get(FileSystem.java:3521)
[2026-02-06T17:24:01.078+0000] {subprocess.py:93} INFO - 	at org.apache.hadoop.fs.FileSystem.get(FileSystem.java:540)
[2026-02-06T17:24:01.079+0000] {subprocess.py:93} INFO - 	at org.apache.hadoop.fs.FileSystem.getLocal(FileSystem.java:496)
[2026-02-06T17:24:01.079+0000] {subprocess.py:93} INFO - 	at org.apache.hadoop.fs.LocalDirAllocator$AllocatorPerContext.confChanged(LocalDirAllocator.java:316)
[2026-02-06T17:24:01.079+0000] {subprocess.py:93} INFO - 	at org.apache.hadoop.fs.LocalDirAllocator$AllocatorPerContext.getLocalPathForWrite(LocalDirAllocator.java:393)
[2026-02-06T17:24:01.079+0000] {subprocess.py:93} INFO - 	at org.apache.hadoop.fs.LocalDirAllocator.getLocalPathForWrite(LocalDirAllocator.java:165)
[2026-02-06T17:24:01.080+0000] {subprocess.py:93} INFO - 	at org.apache.hadoop.fs.LocalDirAllocator.getLocalPathForWrite(LocalDirAllocator.java:146)
[2026-02-06T17:24:01.080+0000] {subprocess.py:93} INFO - 	at org.apache.hadoop.fs.s3a.S3AFileSystem.createTmpFileForWrite(S3AFileSystem.java:1282)
[2026-02-06T17:24:01.080+0000] {subprocess.py:93} INFO - 	at org.apache.hadoop.fs.s3a.S3ADataBlocks$DiskBlockFactory.create(S3ADataBlocks.java:816)
[2026-02-06T17:24:01.080+0000] {subprocess.py:93} INFO - 	at org.apache.hadoop.fs.s3a.S3ABlockOutputStream.createBlockIfNeeded(S3ABlockOutputStream.java:211)
[2026-02-06T17:24:01.080+0000] {subprocess.py:93} INFO - 	at org.apache.hadoop.fs.s3a.S3ABlockOutputStream.<init>(S3ABlockOutputStream.java:188)
[2026-02-06T17:24:01.081+0000] {subprocess.py:93} INFO - 	at org.apache.hadoop.fs.s3a.S3AFileSystem.innerCreateFile(S3AFileSystem.java:1727)
[2026-02-06T17:24:01.081+0000] {subprocess.py:93} INFO - 	at org.apache.hadoop.fs.s3a.S3AFileSystem.lambda$create$6(S3AFileSystem.java:1646)
[2026-02-06T17:24:01.081+0000] {subprocess.py:93} INFO - 	at org.apache.hadoop.fs.statistics.impl.IOStatisticsBinding.lambda$trackDurationOfOperation$5(IOStatisticsBinding.java:499)
[2026-02-06T17:24:01.081+0000] {subprocess.py:93} INFO - 	at org.apache.hadoop.fs.statistics.impl.IOStatisticsBinding.trackDuration(IOStatisticsBinding.java:444)
[2026-02-06T17:24:01.081+0000] {subprocess.py:93} INFO - 	at org.apache.hadoop.fs.s3a.S3AFileSystem.trackDurationAndSpan(S3AFileSystem.java:2337)
[2026-02-06T17:24:01.082+0000] {subprocess.py:93} INFO - 	at org.apache.hadoop.fs.s3a.S3AFileSystem.trackDurationAndSpan(S3AFileSystem.java:2356)
[2026-02-06T17:24:01.083+0000] {subprocess.py:93} INFO - 	at org.apache.hadoop.fs.s3a.S3AFileSystem.create(S3AFileSystem.java:1645)
[2026-02-06T17:24:01.083+0000] {subprocess.py:93} INFO - 	at org.apache.hadoop.fs.FileSystem.create(FileSystem.java:1195)
[2026-02-06T17:24:01.084+0000] {subprocess.py:93} INFO - 	at org.apache.hadoop.fs.FileSystem.create(FileSystem.java:1175)
[2026-02-06T17:24:01.084+0000] {subprocess.py:93} INFO - 	at org.apache.parquet.hadoop.util.HadoopOutputFile.create(HadoopOutputFile.java:74)
[2026-02-06T17:24:01.084+0000] {subprocess.py:93} INFO - 	at org.apache.parquet.hadoop.ParquetFileWriter.<init>(ParquetFileWriter.java:347)
[2026-02-06T17:24:01.084+0000] {subprocess.py:93} INFO - 	at org.apache.parquet.hadoop.ParquetFileWriter.<init>(ParquetFileWriter.java:314)
[2026-02-06T17:24:01.084+0000] {subprocess.py:93} INFO - 	at org.apache.parquet.hadoop.ParquetOutputFormat.getRecordWriter(ParquetOutputFormat.java:484)
[2026-02-06T17:24:01.084+0000] {subprocess.py:93} INFO - 	at org.apache.parquet.hadoop.ParquetOutputFormat.getRecordWriter(ParquetOutputFormat.java:422)
[2026-02-06T17:24:01.085+0000] {subprocess.py:93} INFO - 	at org.apache.parquet.hadoop.ParquetOutputFormat.getRecordWriter(ParquetOutputFormat.java:411)
[2026-02-06T17:24:01.085+0000] {subprocess.py:93} INFO - 	at org.apache.spark.sql.execution.datasources.parquet.ParquetOutputWriter.<init>(ParquetOutputWriter.scala:36)
[2026-02-06T17:24:01.086+0000] {subprocess.py:93} INFO - 	at org.apache.spark.sql.execution.datasources.parquet.ParquetUtils$$anon$1.newInstance(ParquetUtils.scala:490)
[2026-02-06T17:24:01.086+0000] {subprocess.py:93} INFO - 	at org.apache.spark.sql.execution.datasources.SingleDirectoryDataWriter.newOutputWriter(FileFormatDataWriter.scala:161)
[2026-02-06T17:24:01.086+0000] {subprocess.py:93} INFO - 	at org.apache.spark.sql.execution.datasources.SingleDirectoryDataWriter.<init>(FileFormatDataWriter.scala:146)
[2026-02-06T17:24:01.086+0000] {subprocess.py:93} INFO - 	at org.apache.spark.sql.execution.datasources.FileFormatWriter$.executeTask(FileFormatWriter.scala:389)
[2026-02-06T17:24:01.086+0000] {subprocess.py:93} INFO - 	at org.apache.spark.sql.execution.datasources.WriteFilesExec.$anonfun$doExecuteWrite$1(WriteFiles.scala:100)
[2026-02-06T17:24:01.086+0000] {subprocess.py:93} INFO - 	at org.apache.spark.rdd.RDD.$anonfun$mapPartitionsInternal$2(RDD.scala:893)
[2026-02-06T17:24:01.086+0000] {subprocess.py:93} INFO - 	at org.apache.spark.rdd.RDD.$anonfun$mapPartitionsInternal$2$adapted(RDD.scala:893)
[2026-02-06T17:24:01.086+0000] {subprocess.py:93} INFO - 	at org.apache.spark.rdd.MapPartitionsRDD.compute(MapPartitionsRDD.scala:52)
[2026-02-06T17:24:01.087+0000] {subprocess.py:93} INFO - 	at org.apache.spark.rdd.RDD.computeOrReadCheckpoint(RDD.scala:367)
[2026-02-06T17:24:01.087+0000] {subprocess.py:93} INFO - 	at org.apache.spark.rdd.RDD.iterator(RDD.scala:331)
[2026-02-06T17:24:01.087+0000] {subprocess.py:93} INFO - 	at org.apache.spark.scheduler.ResultTask.runTask(ResultTask.scala:93)
[2026-02-06T17:24:01.087+0000] {subprocess.py:93} INFO - 	at org.apache.spark.TaskContext.runTaskWithListeners(TaskContext.scala:166)
[2026-02-06T17:24:01.087+0000] {subprocess.py:93} INFO - 	at org.apache.spark.scheduler.Task.run(Task.scala:141)
[2026-02-06T17:24:01.087+0000] {subprocess.py:93} INFO - 	at org.apache.spark.executor.Executor$TaskRunner.$anonfun$run$4(Executor.scala:620)
[2026-02-06T17:24:01.087+0000] {subprocess.py:93} INFO - 	at org.apache.spark.util.SparkErrorUtils.tryWithSafeFinally(SparkErrorUtils.scala:64)
[2026-02-06T17:24:01.087+0000] {subprocess.py:93} INFO - 	at org.apache.spark.util.SparkErrorUtils.tryWithSafeFinally$(SparkErrorUtils.scala:61)
[2026-02-06T17:24:01.087+0000] {subprocess.py:93} INFO - 	at org.apache.spark.util.Utils$.tryWithSafeFinally(Utils.scala:94)
[2026-02-06T17:24:01.087+0000] {subprocess.py:93} INFO - 	at org.apache.spark.executor.Executor$TaskRunner.run(Executor.scala:623)
[2026-02-06T17:24:01.088+0000] {subprocess.py:93} INFO - 	at java.base/java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1136)
[2026-02-06T17:24:01.088+0000] {subprocess.py:93} INFO - 	at java.base/java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:635)
[2026-02-06T17:24:01.088+0000] {subprocess.py:93} INFO - 	at java.base/java.lang.Thread.run(Thread.java:840)
[2026-02-06T17:24:57.395+0000] {subprocess.py:93} INFO - %d [%thread] %-5level %logger - %msg%n java.io.FileNotFoundException: JAR entry core-default.xml not found in /workspace/ex02_data_ingestion/target/scala-2.13/ex02-ingestion-assembly.jar
[2026-02-06T17:24:57.396+0000] {subprocess.py:93} INFO - 	at sun.net.www.protocol.jar.JarURLConnection.connect(JarURLConnection.java:131) ~[?:?]
[2026-02-06T17:24:57.396+0000] {subprocess.py:93} INFO - 	at sun.net.www.protocol.jar.JarURLConnection.getInputStream(JarURLConnection.java:160) ~[?:?]
[2026-02-06T17:24:57.396+0000] {subprocess.py:93} INFO - 	at org.apache.hadoop.conf.Configuration.parse(Configuration.java:3009) ~[ex02-ingestion-assembly.jar:1.0.0]
[2026-02-06T17:24:57.396+0000] {subprocess.py:93} INFO - 	at org.apache.hadoop.conf.Configuration.getStreamReader(Configuration.java:3105) ~[ex02-ingestion-assembly.jar:1.0.0]
[2026-02-06T17:24:57.396+0000] {subprocess.py:93} INFO - 	at org.apache.hadoop.conf.Configuration.loadResource(Configuration.java:3063) [ex02-ingestion-assembly.jar:1.0.0]
[2026-02-06T17:24:57.396+0000] {subprocess.py:93} INFO - 	at org.apache.hadoop.conf.Configuration.loadResources(Configuration.java:3036) [ex02-ingestion-assembly.jar:1.0.0]
[2026-02-06T17:24:57.396+0000] {subprocess.py:93} INFO - 	at org.apache.hadoop.conf.Configuration.loadProps(Configuration.java:2914) [ex02-ingestion-assembly.jar:1.0.0]
[2026-02-06T17:24:57.396+0000] {subprocess.py:93} INFO - 	at org.apache.hadoop.conf.Configuration.getProps(Configuration.java:2896) [ex02-ingestion-assembly.jar:1.0.0]
[2026-02-06T17:24:57.396+0000] {subprocess.py:93} INFO - 	at org.apache.hadoop.conf.Configuration.get(Configuration.java:1246) [ex02-ingestion-assembly.jar:1.0.0]
[2026-02-06T17:24:57.396+0000] {subprocess.py:93} INFO - 	at org.apache.hadoop.conf.Configuration.getTimeDuration(Configuration.java:1863) [ex02-ingestion-assembly.jar:1.0.0]
[2026-02-06T17:24:57.396+0000] {subprocess.py:93} INFO - 	at org.apache.hadoop.conf.Configuration.getTimeDuration(Configuration.java:1840) [ex02-ingestion-assembly.jar:1.0.0]
[2026-02-06T17:24:57.396+0000] {subprocess.py:93} INFO - 	at org.apache.hadoop.util.ShutdownHookManager.getShutdownTimeout(ShutdownHookManager.java:183) [ex02-ingestion-assembly.jar:1.0.0]
[2026-02-06T17:24:57.396+0000] {subprocess.py:93} INFO - 	at org.apache.hadoop.util.ShutdownHookManager.shutdownExecutor(ShutdownHookManager.java:145) [ex02-ingestion-assembly.jar:1.0.0]
[2026-02-06T17:24:57.396+0000] {subprocess.py:93} INFO - 	at org.apache.hadoop.util.ShutdownHookManager.access$300(ShutdownHookManager.java:65) [ex02-ingestion-assembly.jar:1.0.0]
[2026-02-06T17:24:57.396+0000] {subprocess.py:93} INFO - 	at org.apache.hadoop.util.ShutdownHookManager$1.run(ShutdownHookManager.java:102) [ex02-ingestion-assembly.jar:1.0.0]
[2026-02-06T17:24:57.396+0000] {subprocess.py:93} INFO - Exception in thread "Thread-1" java.lang.RuntimeException: java.io.FileNotFoundException: JAR entry core-default.xml not found in /workspace/ex02_data_ingestion/target/scala-2.13/ex02-ingestion-assembly.jar
[2026-02-06T17:24:57.397+0000] {subprocess.py:93} INFO - 	at org.apache.hadoop.conf.Configuration.loadResource(Configuration.java:3089)
[2026-02-06T17:24:57.397+0000] {subprocess.py:93} INFO - 	at org.apache.hadoop.conf.Configuration.loadResources(Configuration.java:3036)
[2026-02-06T17:24:57.397+0000] {subprocess.py:93} INFO - 	at org.apache.hadoop.conf.Configuration.loadProps(Configuration.java:2914)
[2026-02-06T17:24:57.397+0000] {subprocess.py:93} INFO - 	at org.apache.hadoop.conf.Configuration.getProps(Configuration.java:2896)
[2026-02-06T17:24:57.397+0000] {subprocess.py:93} INFO - 	at org.apache.hadoop.conf.Configuration.get(Configuration.java:1246)
[2026-02-06T17:24:57.397+0000] {subprocess.py:93} INFO - 	at org.apache.hadoop.conf.Configuration.getTimeDuration(Configuration.java:1863)
[2026-02-06T17:24:57.397+0000] {subprocess.py:93} INFO - 	at org.apache.hadoop.conf.Configuration.getTimeDuration(Configuration.java:1840)
[2026-02-06T17:24:57.397+0000] {subprocess.py:93} INFO - 	at org.apache.hadoop.util.ShutdownHookManager.getShutdownTimeout(ShutdownHookManager.java:183)
[2026-02-06T17:24:57.397+0000] {subprocess.py:93} INFO - 	at org.apache.hadoop.util.ShutdownHookManager.shutdownExecutor(ShutdownHookManager.java:145)
[2026-02-06T17:24:57.397+0000] {subprocess.py:93} INFO - 	at org.apache.hadoop.util.ShutdownHookManager.access$300(ShutdownHookManager.java:65)
[2026-02-06T17:24:57.397+0000] {subprocess.py:93} INFO - 	at org.apache.hadoop.util.ShutdownHookManager$1.run(ShutdownHookManager.java:102)
[2026-02-06T17:24:57.397+0000] {subprocess.py:93} INFO - Caused by: java.io.FileNotFoundException: JAR entry core-default.xml not found in /workspace/ex02_data_ingestion/target/scala-2.13/ex02-ingestion-assembly.jar
[2026-02-06T17:24:57.397+0000] {subprocess.py:93} INFO - 	at java.base/sun.net.www.protocol.jar.JarURLConnection.connect(JarURLConnection.java:131)
[2026-02-06T17:24:57.397+0000] {subprocess.py:93} INFO - 	at java.base/sun.net.www.protocol.jar.JarURLConnection.getInputStream(JarURLConnection.java:160)
[2026-02-06T17:24:57.397+0000] {subprocess.py:93} INFO - 	at org.apache.hadoop.conf.Configuration.parse(Configuration.java:3009)
[2026-02-06T17:24:57.397+0000] {subprocess.py:93} INFO - 	at org.apache.hadoop.conf.Configuration.getStreamReader(Configuration.java:3105)
[2026-02-06T17:24:57.397+0000] {subprocess.py:93} INFO - 	at org.apache.hadoop.conf.Configuration.loadResource(Configuration.java:3063)
[2026-02-06T17:24:57.398+0000] {subprocess.py:93} INFO - 	... 10 more
[2026-02-06T17:24:57.427+0000] {subprocess.py:97} INFO - Command exited with return code 1
[2026-02-06T17:24:57.436+0000] {taskinstance.py:3310} ERROR - Task failed with exception
Traceback (most recent call last):
  File "/home/airflow/.local/lib/python3.12/site-packages/airflow/models/taskinstance.py", line 767, in _execute_task
    result = _execute_callable(context=context, **execute_callable_kwargs)
             ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/home/airflow/.local/lib/python3.12/site-packages/airflow/models/taskinstance.py", line 733, in _execute_callable
    return ExecutionCallableRunner(
           ^^^^^^^^^^^^^^^^^^^^^^^^
  File "/home/airflow/.local/lib/python3.12/site-packages/airflow/utils/operator_helpers.py", line 252, in run
    return self.func(*args, **kwargs)
           ^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/home/airflow/.local/lib/python3.12/site-packages/airflow/models/baseoperator.py", line 406, in wrapper
    return func(self, *args, **kwargs)
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/home/airflow/.local/lib/python3.12/site-packages/airflow/operators/bash.py", line 249, in execute
    raise AirflowException(
airflow.exceptions.AirflowException: Bash command failed. The command returned a non-zero exit code 1.
[2026-02-06T17:24:57.443+0000] {taskinstance.py:1225} INFO - Marking task as FAILED. dag_id=tp_bigdata_pipeline, task_id=ex02_ingestion, run_id=manual__2026-02-06T17:22:34.699305+00:00, execution_date=20260206T172234, start_date=20260206T172352, end_date=20260206T172457
[2026-02-06T17:24:57.459+0000] {taskinstance.py:340} INFO - ::group::Post task execution logs
[2026-02-06T17:24:57.460+0000] {standard_task_runner.py:124} ERROR - Failed to execute job 60 for task ex02_ingestion (Bash command failed. The command returned a non-zero exit code 1.; 239)
Traceback (most recent call last):
  File "/home/airflow/.local/lib/python3.12/site-packages/airflow/task/task_runner/standard_task_runner.py", line 117, in _start_by_fork
    ret = args.func(args, dag=self.dag)
          ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/home/airflow/.local/lib/python3.12/site-packages/airflow/cli/cli_config.py", line 49, in command
    return func(*args, **kwargs)
           ^^^^^^^^^^^^^^^^^^^^^
  File "/home/airflow/.local/lib/python3.12/site-packages/airflow/utils/cli.py", line 115, in wrapper
    return f(*args, **kwargs)
           ^^^^^^^^^^^^^^^^^^
  File "/home/airflow/.local/lib/python3.12/site-packages/airflow/cli/commands/task_command.py", line 483, in task_run
    task_return_code = _run_task_by_selected_method(args, _dag, ti)
                       ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/home/airflow/.local/lib/python3.12/site-packages/airflow/cli/commands/task_command.py", line 256, in _run_task_by_selected_method
    return _run_raw_task(args, ti)
           ^^^^^^^^^^^^^^^^^^^^^^^
  File "/home/airflow/.local/lib/python3.12/site-packages/airflow/cli/commands/task_command.py", line 341, in _run_raw_task
    return ti._run_raw_task(
           ^^^^^^^^^^^^^^^^^
  File "/home/airflow/.local/lib/python3.12/site-packages/airflow/utils/session.py", line 97, in wrapper
    return func(*args, session=session, **kwargs)
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/home/airflow/.local/lib/python3.12/site-packages/airflow/models/taskinstance.py", line 3004, in _run_raw_task
    return _run_raw_task(
           ^^^^^^^^^^^^^^
  File "/home/airflow/.local/lib/python3.12/site-packages/airflow/models/taskinstance.py", line 273, in _run_raw_task
    TaskInstance._execute_task_with_callbacks(
  File "/home/airflow/.local/lib/python3.12/site-packages/airflow/models/taskinstance.py", line 3158, in _execute_task_with_callbacks
    result = self._execute_task(context, task_orig)
             ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/home/airflow/.local/lib/python3.12/site-packages/airflow/models/taskinstance.py", line 3182, in _execute_task
    return _execute_task(self, context, task_orig)
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/home/airflow/.local/lib/python3.12/site-packages/airflow/models/taskinstance.py", line 767, in _execute_task
    result = _execute_callable(context=context, **execute_callable_kwargs)
             ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/home/airflow/.local/lib/python3.12/site-packages/airflow/models/taskinstance.py", line 733, in _execute_callable
    return ExecutionCallableRunner(
           ^^^^^^^^^^^^^^^^^^^^^^^^
  File "/home/airflow/.local/lib/python3.12/site-packages/airflow/utils/operator_helpers.py", line 252, in run
    return self.func(*args, **kwargs)
           ^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/home/airflow/.local/lib/python3.12/site-packages/airflow/models/baseoperator.py", line 406, in wrapper
    return func(self, *args, **kwargs)
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/home/airflow/.local/lib/python3.12/site-packages/airflow/operators/bash.py", line 249, in execute
    raise AirflowException(
airflow.exceptions.AirflowException: Bash command failed. The command returned a non-zero exit code 1.
[2026-02-06T17:24:57.488+0000] {local_task_job_runner.py:266} INFO - Task exited with return code 1
[2026-02-06T17:24:57.502+0000] {taskinstance.py:3900} INFO - 0 downstream tasks scheduled from follow-on schedule check
[2026-02-06T17:24:57.509+0000] {local_task_job_runner.py:245} INFO - ::endgroup::
