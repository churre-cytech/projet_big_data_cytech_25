[2026-02-06T17:20:06.736+0000] {local_task_job_runner.py:123} INFO - ::group::Pre task execution logs
[2026-02-06T17:20:06.750+0000] {taskinstance.py:2612} INFO - Dependencies all met for dep_context=non-requeueable deps ti=<TaskInstance: tp_bigdata_pipeline.ex02_ingestion manual__2026-02-06T17:18:48.245383+00:00 [queued]>
[2026-02-06T17:20:06.755+0000] {taskinstance.py:2612} INFO - Dependencies all met for dep_context=requeueable deps ti=<TaskInstance: tp_bigdata_pipeline.ex02_ingestion manual__2026-02-06T17:18:48.245383+00:00 [queued]>
[2026-02-06T17:20:06.756+0000] {taskinstance.py:2865} INFO - Starting attempt 1 of 1
[2026-02-06T17:20:06.775+0000] {taskinstance.py:2888} INFO - Executing <Task(BashOperator): ex02_ingestion> on 2026-02-06 17:18:48.245383+00:00
[2026-02-06T17:20:06.780+0000] {standard_task_runner.py:72} INFO - Started process 730 to run task
[2026-02-06T17:20:06.783+0000] {standard_task_runner.py:104} INFO - Running: ['airflow', 'tasks', 'run', 'tp_bigdata_pipeline', 'ex02_ingestion', 'manual__2026-02-06T17:18:48.245383+00:00', '--job-id', '54', '--raw', '--subdir', 'DAGS_FOLDER/airflow_pipeline.py', '--cfg-path', '/tmp/tmp57j5bhqy']
[2026-02-06T17:20:06.784+0000] {standard_task_runner.py:105} INFO - Job 54: Subtask ex02_ingestion
[2026-02-06T17:20:06.823+0000] {task_command.py:467} INFO - Running <TaskInstance: tp_bigdata_pipeline.ex02_ingestion manual__2026-02-06T17:18:48.245383+00:00 [running]> on host 4d21249f976f
[2026-02-06T17:20:06.891+0000] {taskinstance.py:3131} INFO - Exporting env vars: AIRFLOW_CTX_DAG_OWNER='airflow' AIRFLOW_CTX_DAG_ID='tp_bigdata_pipeline' AIRFLOW_CTX_TASK_ID='ex02_ingestion' AIRFLOW_CTX_EXECUTION_DATE='2026-02-06T17:18:48.245383+00:00' AIRFLOW_CTX_TRY_NUMBER='1' AIRFLOW_CTX_DAG_RUN_ID='manual__2026-02-06T17:18:48.245383+00:00'
[2026-02-06T17:20:06.891+0000] {taskinstance.py:731} INFO - ::endgroup::
[2026-02-06T17:20:06.903+0000] {subprocess.py:63} INFO - Tmp dir root location: /tmp
[2026-02-06T17:20:06.904+0000] {subprocess.py:75} INFO - Running command: ['/usr/bin/bash', '-c', '\n            set -euo pipefail\n            test -f /workspace/ex02_data_ingestion/target/scala-2.13/ex02-ingestion-assembly.jar\n            java --add-exports=java.base/sun.nio.ch=ALL-UNNAMED             --add-exports=java.base/sun.util.calendar=ALL-UNNAMED             -jar /workspace/ex02_data_ingestion/target/scala-2.13/ex02-ingestion-assembly.jar\n        ']
[2026-02-06T17:20:06.911+0000] {subprocess.py:86} INFO - Output:
[2026-02-06T17:20:07.414+0000] {subprocess.py:93} INFO - WARNING: sun.reflect.Reflection.getCallerClass is not supported. This will impact performance.
[2026-02-06T17:20:07.479+0000] {subprocess.py:93} INFO - ERROR StatusLogger Unrecognized format specifier [d]
[2026-02-06T17:20:07.479+0000] {subprocess.py:93} INFO - ERROR StatusLogger Unrecognized conversion specifier [d] starting at position 16 in conversion pattern.
[2026-02-06T17:20:07.481+0000] {subprocess.py:93} INFO - ERROR StatusLogger Unrecognized format specifier [thread]
[2026-02-06T17:20:07.481+0000] {subprocess.py:93} INFO - ERROR StatusLogger Unrecognized conversion specifier [thread] starting at position 25 in conversion pattern.
[2026-02-06T17:20:07.481+0000] {subprocess.py:93} INFO - ERROR StatusLogger Unrecognized format specifier [level]
[2026-02-06T17:20:07.481+0000] {subprocess.py:93} INFO - ERROR StatusLogger Unrecognized conversion specifier [level] starting at position 35 in conversion pattern.
[2026-02-06T17:20:07.481+0000] {subprocess.py:93} INFO - ERROR StatusLogger Unrecognized format specifier [logger]
[2026-02-06T17:20:07.481+0000] {subprocess.py:93} INFO - ERROR StatusLogger Unrecognized conversion specifier [logger] starting at position 47 in conversion pattern.
[2026-02-06T17:20:07.481+0000] {subprocess.py:93} INFO - ERROR StatusLogger Unrecognized format specifier [msg]
[2026-02-06T17:20:07.481+0000] {subprocess.py:93} INFO - ERROR StatusLogger Unrecognized conversion specifier [msg] starting at position 54 in conversion pattern.
[2026-02-06T17:20:07.482+0000] {subprocess.py:93} INFO - ERROR StatusLogger Unrecognized format specifier [n]
[2026-02-06T17:20:07.482+0000] {subprocess.py:93} INFO - ERROR StatusLogger Unrecognized conversion specifier [n] starting at position 56 in conversion pattern.
[2026-02-06T17:20:07.492+0000] {subprocess.py:93} INFO - ERROR StatusLogger Unrecognized format specifier [d]
[2026-02-06T17:20:07.492+0000] {subprocess.py:93} INFO - ERROR StatusLogger Unrecognized conversion specifier [d] starting at position 16 in conversion pattern.
[2026-02-06T17:20:07.492+0000] {subprocess.py:93} INFO - ERROR StatusLogger Unrecognized format specifier [thread]
[2026-02-06T17:20:07.493+0000] {subprocess.py:93} INFO - ERROR StatusLogger Unrecognized conversion specifier [thread] starting at position 25 in conversion pattern.
[2026-02-06T17:20:07.493+0000] {subprocess.py:93} INFO - ERROR StatusLogger Unrecognized format specifier [level]
[2026-02-06T17:20:07.493+0000] {subprocess.py:93} INFO - ERROR StatusLogger Unrecognized conversion specifier [level] starting at position 35 in conversion pattern.
[2026-02-06T17:20:07.493+0000] {subprocess.py:93} INFO - ERROR StatusLogger Unrecognized format specifier [logger]
[2026-02-06T17:20:07.493+0000] {subprocess.py:93} INFO - ERROR StatusLogger Unrecognized conversion specifier [logger] starting at position 47 in conversion pattern.
[2026-02-06T17:20:07.493+0000] {subprocess.py:93} INFO - ERROR StatusLogger Unrecognized format specifier [msg]
[2026-02-06T17:20:07.493+0000] {subprocess.py:93} INFO - ERROR StatusLogger Unrecognized conversion specifier [msg] starting at position 54 in conversion pattern.
[2026-02-06T17:20:07.493+0000] {subprocess.py:93} INFO - ERROR StatusLogger Unrecognized format specifier [n]
[2026-02-06T17:20:07.493+0000] {subprocess.py:93} INFO - ERROR StatusLogger Unrecognized conversion specifier [n] starting at position 56 in conversion pattern.
[2026-02-06T17:20:07.691+0000] {subprocess.py:93} INFO - ERROR StatusLogger Unrecognized format specifier [d]
[2026-02-06T17:20:07.691+0000] {subprocess.py:93} INFO - ERROR StatusLogger Unrecognized conversion specifier [d] starting at position 16 in conversion pattern.
[2026-02-06T17:20:07.691+0000] {subprocess.py:93} INFO - ERROR StatusLogger Unrecognized format specifier [thread]
[2026-02-06T17:20:07.691+0000] {subprocess.py:93} INFO - ERROR StatusLogger Unrecognized conversion specifier [thread] starting at position 25 in conversion pattern.
[2026-02-06T17:20:07.691+0000] {subprocess.py:93} INFO - ERROR StatusLogger Unrecognized format specifier [level]
[2026-02-06T17:20:07.691+0000] {subprocess.py:93} INFO - ERROR StatusLogger Unrecognized conversion specifier [level] starting at position 35 in conversion pattern.
[2026-02-06T17:20:07.691+0000] {subprocess.py:93} INFO - ERROR StatusLogger Unrecognized format specifier [logger]
[2026-02-06T17:20:07.692+0000] {subprocess.py:93} INFO - ERROR StatusLogger Unrecognized conversion specifier [logger] starting at position 47 in conversion pattern.
[2026-02-06T17:20:07.692+0000] {subprocess.py:93} INFO - ERROR StatusLogger Unrecognized format specifier [msg]
[2026-02-06T17:20:07.692+0000] {subprocess.py:93} INFO - ERROR StatusLogger Unrecognized conversion specifier [msg] starting at position 54 in conversion pattern.
[2026-02-06T17:20:07.692+0000] {subprocess.py:93} INFO - ERROR StatusLogger Unrecognized format specifier [n]
[2026-02-06T17:20:07.692+0000] {subprocess.py:93} INFO - ERROR StatusLogger Unrecognized conversion specifier [n] starting at position 56 in conversion pattern.
[2026-02-06T17:20:07.692+0000] {subprocess.py:93} INFO - Using Spark's default log4j profile: org/apache/spark/log4j2-defaults.properties
[2026-02-06T17:20:12.968+0000] {subprocess.py:93} INFO - %d [%thread] %-5level %logger - %msg%n org.apache.hadoop.fs.UnsupportedFileSystemException: No FileSystem for scheme "file"
[2026-02-06T17:20:12.969+0000] {subprocess.py:93} INFO - 	at org.apache.hadoop.fs.FileSystem.getFileSystemClass(FileSystem.java:3443) ~[ex02-ingestion-assembly.jar:1.0.0]
[2026-02-06T17:20:12.969+0000] {subprocess.py:93} INFO - 	at org.apache.hadoop.fs.FileSystem.createFileSystem(FileSystem.java:3466) ~[ex02-ingestion-assembly.jar:1.0.0]
[2026-02-06T17:20:12.969+0000] {subprocess.py:93} INFO - 	at org.apache.hadoop.fs.FileSystem.access$300(FileSystem.java:174) ~[ex02-ingestion-assembly.jar:1.0.0]
[2026-02-06T17:20:12.970+0000] {subprocess.py:93} INFO - 	at org.apache.hadoop.fs.FileSystem$Cache.getInternal(FileSystem.java:3574) ~[ex02-ingestion-assembly.jar:1.0.0]
[2026-02-06T17:20:12.970+0000] {subprocess.py:93} INFO - 	at org.apache.hadoop.fs.FileSystem$Cache.get(FileSystem.java:3521) ~[ex02-ingestion-assembly.jar:1.0.0]
[2026-02-06T17:20:12.970+0000] {subprocess.py:93} INFO - 	at org.apache.hadoop.fs.FileSystem.get(FileSystem.java:540) ~[ex02-ingestion-assembly.jar:1.0.0]
[2026-02-06T17:20:12.970+0000] {subprocess.py:93} INFO - 	at org.apache.hadoop.fs.FileSystem.getLocal(FileSystem.java:496) ~[ex02-ingestion-assembly.jar:1.0.0]
[2026-02-06T17:20:12.970+0000] {subprocess.py:93} INFO - 	at org.apache.hadoop.fs.LocalDirAllocator$AllocatorPerContext.confChanged(LocalDirAllocator.java:316) ~[ex02-ingestion-assembly.jar:1.0.0]
[2026-02-06T17:20:12.970+0000] {subprocess.py:93} INFO - 	at org.apache.hadoop.fs.LocalDirAllocator$AllocatorPerContext.getLocalPathForWrite(LocalDirAllocator.java:393) ~[ex02-ingestion-assembly.jar:1.0.0]
[2026-02-06T17:20:12.970+0000] {subprocess.py:93} INFO - 	at org.apache.hadoop.fs.LocalDirAllocator.getLocalPathForWrite(LocalDirAllocator.java:165) ~[ex02-ingestion-assembly.jar:1.0.0]
[2026-02-06T17:20:12.970+0000] {subprocess.py:93} INFO - 	at org.apache.hadoop.fs.LocalDirAllocator.getLocalPathForWrite(LocalDirAllocator.java:146) ~[ex02-ingestion-assembly.jar:1.0.0]
[2026-02-06T17:20:12.970+0000] {subprocess.py:93} INFO - 	at org.apache.hadoop.fs.s3a.S3AFileSystem.createTmpFileForWrite(S3AFileSystem.java:1282) ~[ex02-ingestion-assembly.jar:1.0.0]
[2026-02-06T17:20:12.971+0000] {subprocess.py:93} INFO - 	at org.apache.hadoop.fs.s3a.S3ADataBlocks$DiskBlockFactory.create(S3ADataBlocks.java:816) ~[ex02-ingestion-assembly.jar:1.0.0]
[2026-02-06T17:20:12.971+0000] {subprocess.py:93} INFO - 	at org.apache.hadoop.fs.s3a.S3ABlockOutputStream.createBlockIfNeeded(S3ABlockOutputStream.java:211) ~[ex02-ingestion-assembly.jar:1.0.0]
[2026-02-06T17:20:12.971+0000] {subprocess.py:93} INFO - 	at org.apache.hadoop.fs.s3a.S3ABlockOutputStream.<init>(S3ABlockOutputStream.java:188) ~[ex02-ingestion-assembly.jar:1.0.0]
[2026-02-06T17:20:12.971+0000] {subprocess.py:93} INFO - 	at org.apache.hadoop.fs.s3a.S3AFileSystem.innerCreateFile(S3AFileSystem.java:1727) ~[ex02-ingestion-assembly.jar:1.0.0]
[2026-02-06T17:20:12.971+0000] {subprocess.py:93} INFO - 	at org.apache.hadoop.fs.s3a.S3AFileSystem.lambda$create$6(S3AFileSystem.java:1646) ~[ex02-ingestion-assembly.jar:1.0.0]
[2026-02-06T17:20:12.971+0000] {subprocess.py:93} INFO - 	at org.apache.hadoop.fs.statistics.impl.IOStatisticsBinding.lambda$trackDurationOfOperation$5(IOStatisticsBinding.java:499) ~[ex02-ingestion-assembly.jar:1.0.0]
[2026-02-06T17:20:12.971+0000] {subprocess.py:93} INFO - 	at org.apache.hadoop.fs.statistics.impl.IOStatisticsBinding.trackDuration(IOStatisticsBinding.java:444) ~[ex02-ingestion-assembly.jar:1.0.0]
[2026-02-06T17:20:12.971+0000] {subprocess.py:93} INFO - 	at org.apache.hadoop.fs.s3a.S3AFileSystem.trackDurationAndSpan(S3AFileSystem.java:2337) ~[ex02-ingestion-assembly.jar:1.0.0]
[2026-02-06T17:20:12.971+0000] {subprocess.py:93} INFO - 	at org.apache.hadoop.fs.s3a.S3AFileSystem.trackDurationAndSpan(S3AFileSystem.java:2356) ~[ex02-ingestion-assembly.jar:1.0.0]
[2026-02-06T17:20:12.971+0000] {subprocess.py:93} INFO - 	at org.apache.hadoop.fs.s3a.S3AFileSystem.create(S3AFileSystem.java:1645) ~[ex02-ingestion-assembly.jar:1.0.0]
[2026-02-06T17:20:12.971+0000] {subprocess.py:93} INFO - 	at org.apache.hadoop.fs.FileSystem.create(FileSystem.java:1195) ~[ex02-ingestion-assembly.jar:1.0.0]
[2026-02-06T17:20:12.971+0000] {subprocess.py:93} INFO - 	at org.apache.hadoop.fs.FileSystem.create(FileSystem.java:1175) ~[ex02-ingestion-assembly.jar:1.0.0]
[2026-02-06T17:20:12.972+0000] {subprocess.py:93} INFO - 	at org.apache.parquet.hadoop.util.HadoopOutputFile.create(HadoopOutputFile.java:74) ~[ex02-ingestion-assembly.jar:1.0.0]
[2026-02-06T17:20:12.972+0000] {subprocess.py:93} INFO - 	at org.apache.parquet.hadoop.ParquetFileWriter.<init>(ParquetFileWriter.java:347) ~[ex02-ingestion-assembly.jar:1.0.0]
[2026-02-06T17:20:12.972+0000] {subprocess.py:93} INFO - 	at org.apache.parquet.hadoop.ParquetFileWriter.<init>(ParquetFileWriter.java:314) ~[ex02-ingestion-assembly.jar:1.0.0]
[2026-02-06T17:20:12.972+0000] {subprocess.py:93} INFO - 	at org.apache.parquet.hadoop.ParquetOutputFormat.getRecordWriter(ParquetOutputFormat.java:484) ~[ex02-ingestion-assembly.jar:1.0.0]
[2026-02-06T17:20:12.972+0000] {subprocess.py:93} INFO - 	at org.apache.parquet.hadoop.ParquetOutputFormat.getRecordWriter(ParquetOutputFormat.java:422) ~[ex02-ingestion-assembly.jar:1.0.0]
[2026-02-06T17:20:12.972+0000] {subprocess.py:93} INFO - 	at org.apache.parquet.hadoop.ParquetOutputFormat.getRecordWriter(ParquetOutputFormat.java:411) ~[ex02-ingestion-assembly.jar:1.0.0]
[2026-02-06T17:20:12.972+0000] {subprocess.py:93} INFO - 	at org.apache.spark.sql.execution.datasources.parquet.ParquetOutputWriter.<init>(ParquetOutputWriter.scala:36) ~[ex02-ingestion-assembly.jar:1.0.0]
[2026-02-06T17:20:12.973+0000] {subprocess.py:93} INFO - 	at org.apache.spark.sql.execution.datasources.parquet.ParquetUtils$$anon$1.newInstance(ParquetUtils.scala:490) ~[ex02-ingestion-assembly.jar:1.0.0]
[2026-02-06T17:20:12.973+0000] {subprocess.py:93} INFO - 	at org.apache.spark.sql.execution.datasources.SingleDirectoryDataWriter.newOutputWriter(FileFormatDataWriter.scala:161) ~[ex02-ingestion-assembly.jar:1.0.0]
[2026-02-06T17:20:12.973+0000] {subprocess.py:93} INFO - 	at org.apache.spark.sql.execution.datasources.SingleDirectoryDataWriter.<init>(FileFormatDataWriter.scala:146) ~[ex02-ingestion-assembly.jar:1.0.0]
[2026-02-06T17:20:12.973+0000] {subprocess.py:93} INFO - 	at org.apache.spark.sql.execution.datasources.FileFormatWriter$.executeTask(FileFormatWriter.scala:389) ~[ex02-ingestion-assembly.jar:1.0.0]
[2026-02-06T17:20:12.973+0000] {subprocess.py:93} INFO - 	at org.apache.spark.sql.execution.datasources.WriteFilesExec.$anonfun$doExecuteWrite$1(WriteFiles.scala:100) ~[ex02-ingestion-assembly.jar:1.0.0]
[2026-02-06T17:20:12.973+0000] {subprocess.py:93} INFO - 	at org.apache.spark.rdd.RDD.$anonfun$mapPartitionsInternal$2(RDD.scala:893) ~[ex02-ingestion-assembly.jar:1.0.0]
[2026-02-06T17:20:12.973+0000] {subprocess.py:93} INFO - 	at org.apache.spark.rdd.RDD.$anonfun$mapPartitionsInternal$2$adapted(RDD.scala:893) ~[ex02-ingestion-assembly.jar:1.0.0]
[2026-02-06T17:20:12.974+0000] {subprocess.py:93} INFO - 	at org.apache.spark.rdd.MapPartitionsRDD.compute(MapPartitionsRDD.scala:52) ~[ex02-ingestion-assembly.jar:1.0.0]
[2026-02-06T17:20:12.974+0000] {subprocess.py:93} INFO - 	at org.apache.spark.rdd.RDD.computeOrReadCheckpoint(RDD.scala:367) ~[ex02-ingestion-assembly.jar:1.0.0]
[2026-02-06T17:20:12.974+0000] {subprocess.py:93} INFO - 	at org.apache.spark.rdd.RDD.iterator(RDD.scala:331) ~[ex02-ingestion-assembly.jar:1.0.0]
[2026-02-06T17:20:12.974+0000] {subprocess.py:93} INFO - 	at org.apache.spark.scheduler.ResultTask.runTask(ResultTask.scala:93) ~[ex02-ingestion-assembly.jar:1.0.0]
[2026-02-06T17:20:12.974+0000] {subprocess.py:93} INFO - 	at org.apache.spark.TaskContext.runTaskWithListeners(TaskContext.scala:166) ~[ex02-ingestion-assembly.jar:1.0.0]
[2026-02-06T17:20:12.974+0000] {subprocess.py:93} INFO - 	at org.apache.spark.scheduler.Task.run(Task.scala:141) ~[ex02-ingestion-assembly.jar:1.0.0]
[2026-02-06T17:20:12.974+0000] {subprocess.py:93} INFO - 	at org.apache.spark.executor.Executor$TaskRunner.$anonfun$run$4(Executor.scala:620) ~[ex02-ingestion-assembly.jar:1.0.0]
[2026-02-06T17:20:12.974+0000] {subprocess.py:93} INFO - 	at org.apache.spark.util.SparkErrorUtils.tryWithSafeFinally(SparkErrorUtils.scala:64) ~[ex02-ingestion-assembly.jar:1.0.0]
[2026-02-06T17:20:12.974+0000] {subprocess.py:93} INFO - 	at org.apache.spark.util.SparkErrorUtils.tryWithSafeFinally$(SparkErrorUtils.scala:61) ~[ex02-ingestion-assembly.jar:1.0.0]
[2026-02-06T17:20:12.975+0000] {subprocess.py:93} INFO - 	at org.apache.spark.util.Utils$.tryWithSafeFinally(Utils.scala:94) ~[ex02-ingestion-assembly.jar:1.0.0]
[2026-02-06T17:20:12.975+0000] {subprocess.py:93} INFO - 	at org.apache.spark.executor.Executor$TaskRunner.run(Executor.scala:623) [ex02-ingestion-assembly.jar:1.0.0]
[2026-02-06T17:20:12.975+0000] {subprocess.py:93} INFO - 	at java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1136) [?:?]
[2026-02-06T17:20:12.975+0000] {subprocess.py:93} INFO - 	at java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:635) [?:?]
[2026-02-06T17:20:12.975+0000] {subprocess.py:93} INFO - 	at java.lang.Thread.run(Thread.java:840) [?:?]
[2026-02-06T17:20:13.013+0000] {subprocess.py:93} INFO - %d [%thread] %-5level %logger - %msg%n%d [%thread] %-5level %logger - %msg%n org.apache.spark.SparkException: Job aborted due to stage failure: Task 0 in stage 1.0 failed 1 times, most recent failure: Lost task 0.0 in stage 1.0 (TID 1) (4d21249f976f executor driver): org.apache.hadoop.fs.UnsupportedFileSystemException: No FileSystem for scheme "file"
[2026-02-06T17:20:13.014+0000] {subprocess.py:93} INFO - 	at org.apache.hadoop.fs.FileSystem.getFileSystemClass(FileSystem.java:3443)
[2026-02-06T17:20:13.014+0000] {subprocess.py:93} INFO - 	at org.apache.hadoop.fs.FileSystem.createFileSystem(FileSystem.java:3466)
[2026-02-06T17:20:13.014+0000] {subprocess.py:93} INFO - 	at org.apache.hadoop.fs.FileSystem.access$300(FileSystem.java:174)
[2026-02-06T17:20:13.014+0000] {subprocess.py:93} INFO - 	at org.apache.hadoop.fs.FileSystem$Cache.getInternal(FileSystem.java:3574)
[2026-02-06T17:20:13.015+0000] {subprocess.py:93} INFO - 	at org.apache.hadoop.fs.FileSystem$Cache.get(FileSystem.java:3521)
[2026-02-06T17:20:13.015+0000] {subprocess.py:93} INFO - 	at org.apache.hadoop.fs.FileSystem.get(FileSystem.java:540)
[2026-02-06T17:20:13.015+0000] {subprocess.py:93} INFO - 	at org.apache.hadoop.fs.FileSystem.getLocal(FileSystem.java:496)
[2026-02-06T17:20:13.015+0000] {subprocess.py:93} INFO - 	at org.apache.hadoop.fs.LocalDirAllocator$AllocatorPerContext.confChanged(LocalDirAllocator.java:316)
[2026-02-06T17:20:13.015+0000] {subprocess.py:93} INFO - 	at org.apache.hadoop.fs.LocalDirAllocator$AllocatorPerContext.getLocalPathForWrite(LocalDirAllocator.java:393)
[2026-02-06T17:20:13.015+0000] {subprocess.py:93} INFO - 	at org.apache.hadoop.fs.LocalDirAllocator.getLocalPathForWrite(LocalDirAllocator.java:165)
[2026-02-06T17:20:13.015+0000] {subprocess.py:93} INFO - 	at org.apache.hadoop.fs.LocalDirAllocator.getLocalPathForWrite(LocalDirAllocator.java:146)
[2026-02-06T17:20:13.016+0000] {subprocess.py:93} INFO - 	at org.apache.hadoop.fs.s3a.S3AFileSystem.createTmpFileForWrite(S3AFileSystem.java:1282)
[2026-02-06T17:20:13.016+0000] {subprocess.py:93} INFO - 	at org.apache.hadoop.fs.s3a.S3ADataBlocks$DiskBlockFactory.create(S3ADataBlocks.java:816)
[2026-02-06T17:20:13.016+0000] {subprocess.py:93} INFO - 	at org.apache.hadoop.fs.s3a.S3ABlockOutputStream.createBlockIfNeeded(S3ABlockOutputStream.java:211)
[2026-02-06T17:20:13.016+0000] {subprocess.py:93} INFO - 	at org.apache.hadoop.fs.s3a.S3ABlockOutputStream.<init>(S3ABlockOutputStream.java:188)
[2026-02-06T17:20:13.016+0000] {subprocess.py:93} INFO - 	at org.apache.hadoop.fs.s3a.S3AFileSystem.innerCreateFile(S3AFileSystem.java:1727)
[2026-02-06T17:20:13.016+0000] {subprocess.py:93} INFO - 	at org.apache.hadoop.fs.s3a.S3AFileSystem.lambda$create$6(S3AFileSystem.java:1646)
[2026-02-06T17:20:13.016+0000] {subprocess.py:93} INFO - 	at org.apache.hadoop.fs.statistics.impl.IOStatisticsBinding.lambda$trackDurationOfOperation$5(IOStatisticsBinding.java:499)
[2026-02-06T17:20:13.016+0000] {subprocess.py:93} INFO - 	at org.apache.hadoop.fs.statistics.impl.IOStatisticsBinding.trackDuration(IOStatisticsBinding.java:444)
[2026-02-06T17:20:13.016+0000] {subprocess.py:93} INFO - 	at org.apache.hadoop.fs.s3a.S3AFileSystem.trackDurationAndSpan(S3AFileSystem.java:2337)
[2026-02-06T17:20:13.016+0000] {subprocess.py:93} INFO - 	at org.apache.hadoop.fs.s3a.S3AFileSystem.trackDurationAndSpan(S3AFileSystem.java:2356)
[2026-02-06T17:20:13.017+0000] {subprocess.py:93} INFO - 	at org.apache.hadoop.fs.s3a.S3AFileSystem.create(S3AFileSystem.java:1645)
[2026-02-06T17:20:13.017+0000] {subprocess.py:93} INFO - 	at org.apache.hadoop.fs.FileSystem.create(FileSystem.java:1195)
[2026-02-06T17:20:13.017+0000] {subprocess.py:93} INFO - 	at org.apache.hadoop.fs.FileSystem.create(FileSystem.java:1175)
[2026-02-06T17:20:13.017+0000] {subprocess.py:93} INFO - 	at org.apache.parquet.hadoop.util.HadoopOutputFile.create(HadoopOutputFile.java:74)
[2026-02-06T17:20:13.017+0000] {subprocess.py:93} INFO - 	at org.apache.parquet.hadoop.ParquetFileWriter.<init>(ParquetFileWriter.java:347)
[2026-02-06T17:20:13.017+0000] {subprocess.py:93} INFO - 	at org.apache.parquet.hadoop.ParquetFileWriter.<init>(ParquetFileWriter.java:314)
[2026-02-06T17:20:13.017+0000] {subprocess.py:93} INFO - 	at org.apache.parquet.hadoop.ParquetOutputFormat.getRecordWriter(ParquetOutputFormat.java:484)
[2026-02-06T17:20:13.017+0000] {subprocess.py:93} INFO - 	at org.apache.parquet.hadoop.ParquetOutputFormat.getRecordWriter(ParquetOutputFormat.java:422)
[2026-02-06T17:20:13.017+0000] {subprocess.py:93} INFO - 	at org.apache.parquet.hadoop.ParquetOutputFormat.getRecordWriter(ParquetOutputFormat.java:411)
[2026-02-06T17:20:13.017+0000] {subprocess.py:93} INFO - 	at org.apache.spark.sql.execution.datasources.parquet.ParquetOutputWriter.<init>(ParquetOutputWriter.scala:36)
[2026-02-06T17:20:13.017+0000] {subprocess.py:93} INFO - 	at org.apache.spark.sql.execution.datasources.parquet.ParquetUtils$$anon$1.newInstance(ParquetUtils.scala:490)
[2026-02-06T17:20:13.017+0000] {subprocess.py:93} INFO - 	at org.apache.spark.sql.execution.datasources.SingleDirectoryDataWriter.newOutputWriter(FileFormatDataWriter.scala:161)
[2026-02-06T17:20:13.017+0000] {subprocess.py:93} INFO - 	at org.apache.spark.sql.execution.datasources.SingleDirectoryDataWriter.<init>(FileFormatDataWriter.scala:146)
[2026-02-06T17:20:13.017+0000] {subprocess.py:93} INFO - 	at org.apache.spark.sql.execution.datasources.FileFormatWriter$.executeTask(FileFormatWriter.scala:389)
[2026-02-06T17:20:13.018+0000] {subprocess.py:93} INFO - 	at org.apache.spark.sql.execution.datasources.WriteFilesExec.$anonfun$doExecuteWrite$1(WriteFiles.scala:100)
[2026-02-06T17:20:13.018+0000] {subprocess.py:93} INFO - 	at org.apache.spark.rdd.RDD.$anonfun$mapPartitionsInternal$2(RDD.scala:893)
[2026-02-06T17:20:13.018+0000] {subprocess.py:93} INFO - 	at org.apache.spark.rdd.RDD.$anonfun$mapPartitionsInternal$2$adapted(RDD.scala:893)
[2026-02-06T17:20:13.018+0000] {subprocess.py:93} INFO - 	at org.apache.spark.rdd.MapPartitionsRDD.compute(MapPartitionsRDD.scala:52)
[2026-02-06T17:20:13.018+0000] {subprocess.py:93} INFO - 	at org.apache.spark.rdd.RDD.computeOrReadCheckpoint(RDD.scala:367)
[2026-02-06T17:20:13.018+0000] {subprocess.py:93} INFO - 	at org.apache.spark.rdd.RDD.iterator(RDD.scala:331)
[2026-02-06T17:20:13.018+0000] {subprocess.py:93} INFO - 	at org.apache.spark.scheduler.ResultTask.runTask(ResultTask.scala:93)
[2026-02-06T17:20:13.018+0000] {subprocess.py:93} INFO - 	at org.apache.spark.TaskContext.runTaskWithListeners(TaskContext.scala:166)
[2026-02-06T17:20:13.018+0000] {subprocess.py:93} INFO - 	at org.apache.spark.scheduler.Task.run(Task.scala:141)
[2026-02-06T17:20:13.018+0000] {subprocess.py:93} INFO - 	at org.apache.spark.executor.Executor$TaskRunner.$anonfun$run$4(Executor.scala:620)
[2026-02-06T17:20:13.018+0000] {subprocess.py:93} INFO - 	at org.apache.spark.util.SparkErrorUtils.tryWithSafeFinally(SparkErrorUtils.scala:64)
[2026-02-06T17:20:13.019+0000] {subprocess.py:93} INFO - 	at org.apache.spark.util.SparkErrorUtils.tryWithSafeFinally$(SparkErrorUtils.scala:61)
[2026-02-06T17:20:13.019+0000] {subprocess.py:93} INFO - 	at org.apache.spark.util.Utils$.tryWithSafeFinally(Utils.scala:94)
[2026-02-06T17:20:13.019+0000] {subprocess.py:93} INFO - 	at org.apache.spark.executor.Executor$TaskRunner.run(Executor.scala:623)
[2026-02-06T17:20:13.019+0000] {subprocess.py:93} INFO - 	at java.base/java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1136)
[2026-02-06T17:20:13.019+0000] {subprocess.py:93} INFO - 	at java.base/java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:635)
[2026-02-06T17:20:13.019+0000] {subprocess.py:93} INFO - 	at java.base/java.lang.Thread.run(Thread.java:840)
[2026-02-06T17:20:13.019+0000] {subprocess.py:93} INFO - 
[2026-02-06T17:20:13.019+0000] {subprocess.py:93} INFO - Driver stacktrace:
[2026-02-06T17:20:13.020+0000] {subprocess.py:93} INFO - 	at org.apache.spark.scheduler.DAGScheduler.failJobAndIndependentStages(DAGScheduler.scala:2856) ~[ex02-ingestion-assembly.jar:1.0.0]
[2026-02-06T17:20:13.020+0000] {subprocess.py:93} INFO - 	at org.apache.spark.scheduler.DAGScheduler.$anonfun$abortStage$2(DAGScheduler.scala:2792) ~[ex02-ingestion-assembly.jar:1.0.0]
[2026-02-06T17:20:13.020+0000] {subprocess.py:93} INFO - 	at org.apache.spark.scheduler.DAGScheduler.$anonfun$abortStage$2$adapted(DAGScheduler.scala:2791) ~[ex02-ingestion-assembly.jar:1.0.0]
[2026-02-06T17:20:13.020+0000] {subprocess.py:93} INFO - 	at scala.collection.immutable.List.foreach(List.scala:323) ~[ex02-ingestion-assembly.jar:1.0.0]
[2026-02-06T17:20:13.020+0000] {subprocess.py:93} INFO - 	at org.apache.spark.scheduler.DAGScheduler.abortStage(DAGScheduler.scala:2791) ~[ex02-ingestion-assembly.jar:1.0.0]
[2026-02-06T17:20:13.020+0000] {subprocess.py:93} INFO - 	at org.apache.spark.scheduler.DAGScheduler.$anonfun$handleTaskSetFailed$1(DAGScheduler.scala:1247) ~[ex02-ingestion-assembly.jar:1.0.0]
[2026-02-06T17:20:13.020+0000] {subprocess.py:93} INFO - 	at org.apache.spark.scheduler.DAGScheduler.$anonfun$handleTaskSetFailed$1$adapted(DAGScheduler.scala:1247) ~[ex02-ingestion-assembly.jar:1.0.0]
[2026-02-06T17:20:13.020+0000] {subprocess.py:93} INFO - 	at scala.Option.foreach(Option.scala:437) ~[ex02-ingestion-assembly.jar:1.0.0]
[2026-02-06T17:20:13.020+0000] {subprocess.py:93} INFO - 	at org.apache.spark.scheduler.DAGScheduler.handleTaskSetFailed(DAGScheduler.scala:1247) ~[ex02-ingestion-assembly.jar:1.0.0]
[2026-02-06T17:20:13.020+0000] {subprocess.py:93} INFO - 	at org.apache.spark.scheduler.DAGSchedulerEventProcessLoop.doOnReceive(DAGScheduler.scala:3060) ~[ex02-ingestion-assembly.jar:1.0.0]
[2026-02-06T17:20:13.021+0000] {subprocess.py:93} INFO - 	at org.apache.spark.scheduler.DAGSchedulerEventProcessLoop.onReceive(DAGScheduler.scala:2994) ~[ex02-ingestion-assembly.jar:1.0.0]
[2026-02-06T17:20:13.021+0000] {subprocess.py:93} INFO - 	at org.apache.spark.scheduler.DAGSchedulerEventProcessLoop.onReceive(DAGScheduler.scala:2983) ~[ex02-ingestion-assembly.jar:1.0.0]
[2026-02-06T17:20:13.021+0000] {subprocess.py:93} INFO - 	at org.apache.spark.util.EventLoop$$anon$1.run(EventLoop.scala:49) ~[ex02-ingestion-assembly.jar:1.0.0]
[2026-02-06T17:20:13.021+0000] {subprocess.py:93} INFO - 	at org.apache.spark.scheduler.DAGScheduler.runJob(DAGScheduler.scala:989) ~[ex02-ingestion-assembly.jar:1.0.0]
[2026-02-06T17:20:13.021+0000] {subprocess.py:93} INFO - 	at org.apache.spark.SparkContext.runJob(SparkContext.scala:2393) ~[ex02-ingestion-assembly.jar:1.0.0]
[2026-02-06T17:20:13.021+0000] {subprocess.py:93} INFO - 	at org.apache.spark.sql.execution.datasources.FileFormatWriter$.$anonfun$executeWrite$4(FileFormatWriter.scala:307) ~[ex02-ingestion-assembly.jar:1.0.0]
[2026-02-06T17:20:13.021+0000] {subprocess.py:93} INFO - 	at org.apache.spark.sql.execution.datasources.FileFormatWriter$.writeAndCommit(FileFormatWriter.scala:271) ~[ex02-ingestion-assembly.jar:1.0.0]
[2026-02-06T17:20:13.021+0000] {subprocess.py:93} INFO - 	at org.apache.spark.sql.execution.datasources.FileFormatWriter$.executeWrite(FileFormatWriter.scala:304) ~[ex02-ingestion-assembly.jar:1.0.0]
[2026-02-06T17:20:13.021+0000] {subprocess.py:93} INFO - 	at org.apache.spark.sql.execution.datasources.FileFormatWriter$.write(FileFormatWriter.scala:190) ~[ex02-ingestion-assembly.jar:1.0.0]
[2026-02-06T17:20:13.021+0000] {subprocess.py:93} INFO - 	at org.apache.spark.sql.execution.datasources.InsertIntoHadoopFsRelationCommand.run(InsertIntoHadoopFsRelationCommand.scala:190) ~[ex02-ingestion-assembly.jar:1.0.0]
[2026-02-06T17:20:13.022+0000] {subprocess.py:93} INFO - 	at org.apache.spark.sql.execution.command.DataWritingCommandExec.sideEffectResult$lzycompute(commands.scala:113) ~[ex02-ingestion-assembly.jar:1.0.0]
[2026-02-06T17:20:13.022+0000] {subprocess.py:93} INFO - 	at org.apache.spark.sql.execution.command.DataWritingCommandExec.sideEffectResult(commands.scala:111) ~[ex02-ingestion-assembly.jar:1.0.0]
[2026-02-06T17:20:13.022+0000] {subprocess.py:93} INFO - 	at org.apache.spark.sql.execution.command.DataWritingCommandExec.executeCollect(commands.scala:125) ~[ex02-ingestion-assembly.jar:1.0.0]
[2026-02-06T17:20:13.022+0000] {subprocess.py:93} INFO - 	at org.apache.spark.sql.execution.QueryExecution$$anonfun$eagerlyExecuteCommands$1.$anonfun$applyOrElse$1(QueryExecution.scala:107) ~[ex02-ingestion-assembly.jar:1.0.0]
[2026-02-06T17:20:13.022+0000] {subprocess.py:93} INFO - 	at org.apache.spark.sql.execution.SQLExecution$.$anonfun$withNewExecutionId$6(SQLExecution.scala:125) ~[ex02-ingestion-assembly.jar:1.0.0]
[2026-02-06T17:20:13.022+0000] {subprocess.py:93} INFO - 	at org.apache.spark.sql.execution.SQLExecution$.withSQLConfPropagated(SQLExecution.scala:201) ~[ex02-ingestion-assembly.jar:1.0.0]
[2026-02-06T17:20:13.022+0000] {subprocess.py:93} INFO - 	at org.apache.spark.sql.execution.SQLExecution$.$anonfun$withNewExecutionId$1(SQLExecution.scala:108) ~[ex02-ingestion-assembly.jar:1.0.0]
[2026-02-06T17:20:13.022+0000] {subprocess.py:93} INFO - 	at org.apache.spark.sql.SparkSession.withActive(SparkSession.scala:900) ~[ex02-ingestion-assembly.jar:1.0.0]
[2026-02-06T17:20:13.022+0000] {subprocess.py:93} INFO - 	at org.apache.spark.sql.execution.SQLExecution$.withNewExecutionId(SQLExecution.scala:66) ~[ex02-ingestion-assembly.jar:1.0.0]
[2026-02-06T17:20:13.022+0000] {subprocess.py:93} INFO - 	at org.apache.spark.sql.execution.QueryExecution$$anonfun$eagerlyExecuteCommands$1.applyOrElse(QueryExecution.scala:107) ~[ex02-ingestion-assembly.jar:1.0.0]
[2026-02-06T17:20:13.023+0000] {subprocess.py:93} INFO - 	at org.apache.spark.sql.execution.QueryExecution$$anonfun$eagerlyExecuteCommands$1.applyOrElse(QueryExecution.scala:98) ~[ex02-ingestion-assembly.jar:1.0.0]
[2026-02-06T17:20:13.023+0000] {subprocess.py:93} INFO - 	at org.apache.spark.sql.catalyst.trees.TreeNode.$anonfun$transformDownWithPruning$1(TreeNode.scala:461) ~[ex02-ingestion-assembly.jar:1.0.0]
[2026-02-06T17:20:13.023+0000] {subprocess.py:93} INFO - 	at org.apache.spark.sql.catalyst.trees.CurrentOrigin$.withOrigin(origin.scala:76) [ex02-ingestion-assembly.jar:1.0.0]
[2026-02-06T17:20:13.023+0000] {subprocess.py:93} INFO - 	at org.apache.spark.sql.catalyst.trees.TreeNode.transformDownWithPruning(TreeNode.scala:461) [ex02-ingestion-assembly.jar:1.0.0]
[2026-02-06T17:20:13.023+0000] {subprocess.py:93} INFO - 	at org.apache.spark.sql.catalyst.plans.logical.LogicalPlan.org$apache$spark$sql$catalyst$plans$logical$AnalysisHelper$$super$transformDownWithPruning(LogicalPlan.scala:32) [ex02-ingestion-assembly.jar:1.0.0]
[2026-02-06T17:20:13.023+0000] {subprocess.py:93} INFO - 	at org.apache.spark.sql.catalyst.plans.logical.AnalysisHelper.transformDownWithPruning(AnalysisHelper.scala:267) [ex02-ingestion-assembly.jar:1.0.0]
[2026-02-06T17:20:13.023+0000] {subprocess.py:93} INFO - 	at org.apache.spark.sql.catalyst.plans.logical.AnalysisHelper.transformDownWithPruning$(AnalysisHelper.scala:263) [ex02-ingestion-assembly.jar:1.0.0]
[2026-02-06T17:20:13.023+0000] {subprocess.py:93} INFO - 	at org.apache.spark.sql.catalyst.plans.logical.LogicalPlan.transformDownWithPruning(LogicalPlan.scala:32) [ex02-ingestion-assembly.jar:1.0.0]
[2026-02-06T17:20:13.023+0000] {subprocess.py:93} INFO - 	at org.apache.spark.sql.catalyst.plans.logical.LogicalPlan.transformDownWithPruning(LogicalPlan.scala:32) [ex02-ingestion-assembly.jar:1.0.0]
[2026-02-06T17:20:13.023+0000] {subprocess.py:93} INFO - 	at org.apache.spark.sql.catalyst.trees.TreeNode.transformDown(TreeNode.scala:437) [ex02-ingestion-assembly.jar:1.0.0]
[2026-02-06T17:20:13.023+0000] {subprocess.py:93} INFO - 	at org.apache.spark.sql.execution.QueryExecution.eagerlyExecuteCommands(QueryExecution.scala:98) [ex02-ingestion-assembly.jar:1.0.0]
[2026-02-06T17:20:13.023+0000] {subprocess.py:93} INFO - 	at org.apache.spark.sql.execution.QueryExecution.commandExecuted$lzycompute(QueryExecution.scala:85) [ex02-ingestion-assembly.jar:1.0.0]
[2026-02-06T17:20:13.023+0000] {subprocess.py:93} INFO - 	at org.apache.spark.sql.execution.QueryExecution.commandExecuted(QueryExecution.scala:83) [ex02-ingestion-assembly.jar:1.0.0]
[2026-02-06T17:20:13.023+0000] {subprocess.py:93} INFO - 	at org.apache.spark.sql.execution.QueryExecution.assertCommandExecuted(QueryExecution.scala:142) [ex02-ingestion-assembly.jar:1.0.0]
[2026-02-06T17:20:13.023+0000] {subprocess.py:93} INFO - 	at org.apache.spark.sql.DataFrameWriter.runCommand(DataFrameWriter.scala:869) [ex02-ingestion-assembly.jar:1.0.0]
[2026-02-06T17:20:13.024+0000] {subprocess.py:93} INFO - 	at org.apache.spark.sql.DataFrameWriter.saveToV1Source(DataFrameWriter.scala:391) [ex02-ingestion-assembly.jar:1.0.0]
[2026-02-06T17:20:13.024+0000] {subprocess.py:93} INFO - 	at org.apache.spark.sql.DataFrameWriter.saveInternal(DataFrameWriter.scala:364) [ex02-ingestion-assembly.jar:1.0.0]
[2026-02-06T17:20:13.024+0000] {subprocess.py:93} INFO - 	at org.apache.spark.sql.DataFrameWriter.save(DataFrameWriter.scala:243) [ex02-ingestion-assembly.jar:1.0.0]
[2026-02-06T17:20:13.024+0000] {subprocess.py:93} INFO - 	at org.apache.spark.sql.DataFrameWriter.parquet(DataFrameWriter.scala:802) [ex02-ingestion-assembly.jar:1.0.0]
[2026-02-06T17:20:13.024+0000] {subprocess.py:93} INFO - 	at fr.cytech.ingestion.Main$.main(Main.scala:107) [ex02-ingestion-assembly.jar:1.0.0]
[2026-02-06T17:20:13.024+0000] {subprocess.py:93} INFO - 	at fr.cytech.ingestion.Main.main(Main.scala) [ex02-ingestion-assembly.jar:1.0.0]
[2026-02-06T17:20:13.024+0000] {subprocess.py:93} INFO - Caused by: org.apache.hadoop.fs.UnsupportedFileSystemException: No FileSystem for scheme "file"
[2026-02-06T17:20:13.024+0000] {subprocess.py:93} INFO - 	at org.apache.hadoop.fs.FileSystem.getFileSystemClass(FileSystem.java:3443) ~[ex02-ingestion-assembly.jar:1.0.0]
[2026-02-06T17:20:13.025+0000] {subprocess.py:93} INFO - 	at org.apache.hadoop.fs.FileSystem.createFileSystem(FileSystem.java:3466) ~[ex02-ingestion-assembly.jar:1.0.0]
[2026-02-06T17:20:13.025+0000] {subprocess.py:93} INFO - 	at org.apache.hadoop.fs.FileSystem.access$300(FileSystem.java:174) ~[ex02-ingestion-assembly.jar:1.0.0]
[2026-02-06T17:20:13.025+0000] {subprocess.py:93} INFO - 	at org.apache.hadoop.fs.FileSystem$Cache.getInternal(FileSystem.java:3574) ~[ex02-ingestion-assembly.jar:1.0.0]
[2026-02-06T17:20:13.025+0000] {subprocess.py:93} INFO - 	at org.apache.hadoop.fs.FileSystem$Cache.get(FileSystem.java:3521) ~[ex02-ingestion-assembly.jar:1.0.0]
[2026-02-06T17:20:13.025+0000] {subprocess.py:93} INFO - 	at org.apache.hadoop.fs.FileSystem.get(FileSystem.java:540) ~[ex02-ingestion-assembly.jar:1.0.0]
[2026-02-06T17:20:13.025+0000] {subprocess.py:93} INFO - 	at org.apache.hadoop.fs.FileSystem.getLocal(FileSystem.java:496) ~[ex02-ingestion-assembly.jar:1.0.0]
[2026-02-06T17:20:13.025+0000] {subprocess.py:93} INFO - 	at org.apache.hadoop.fs.LocalDirAllocator$AllocatorPerContext.confChanged(LocalDirAllocator.java:316) ~[ex02-ingestion-assembly.jar:1.0.0]
[2026-02-06T17:20:13.025+0000] {subprocess.py:93} INFO - 	at org.apache.hadoop.fs.LocalDirAllocator$AllocatorPerContext.getLocalPathForWrite(LocalDirAllocator.java:393) ~[ex02-ingestion-assembly.jar:1.0.0]
[2026-02-06T17:20:13.025+0000] {subprocess.py:93} INFO - 	at org.apache.hadoop.fs.LocalDirAllocator.getLocalPathForWrite(LocalDirAllocator.java:165) ~[ex02-ingestion-assembly.jar:1.0.0]
[2026-02-06T17:20:13.026+0000] {subprocess.py:93} INFO - 	at org.apache.hadoop.fs.LocalDirAllocator.getLocalPathForWrite(LocalDirAllocator.java:146) ~[ex02-ingestion-assembly.jar:1.0.0]
[2026-02-06T17:20:13.026+0000] {subprocess.py:93} INFO - 	at org.apache.hadoop.fs.s3a.S3AFileSystem.createTmpFileForWrite(S3AFileSystem.java:1282) ~[ex02-ingestion-assembly.jar:1.0.0]
[2026-02-06T17:20:13.026+0000] {subprocess.py:93} INFO - 	at org.apache.hadoop.fs.s3a.S3ADataBlocks$DiskBlockFactory.create(S3ADataBlocks.java:816) ~[ex02-ingestion-assembly.jar:1.0.0]
[2026-02-06T17:20:13.026+0000] {subprocess.py:93} INFO - 	at org.apache.hadoop.fs.s3a.S3ABlockOutputStream.createBlockIfNeeded(S3ABlockOutputStream.java:211) ~[ex02-ingestion-assembly.jar:1.0.0]
[2026-02-06T17:20:13.026+0000] {subprocess.py:93} INFO - 	at org.apache.hadoop.fs.s3a.S3ABlockOutputStream.<init>(S3ABlockOutputStream.java:188) ~[ex02-ingestion-assembly.jar:1.0.0]
[2026-02-06T17:20:13.026+0000] {subprocess.py:93} INFO - 	at org.apache.hadoop.fs.s3a.S3AFileSystem.innerCreateFile(S3AFileSystem.java:1727) ~[ex02-ingestion-assembly.jar:1.0.0]
[2026-02-06T17:20:13.026+0000] {subprocess.py:93} INFO - 	at org.apache.hadoop.fs.s3a.S3AFileSystem.lambda$create$6(S3AFileSystem.java:1646) ~[ex02-ingestion-assembly.jar:1.0.0]
[2026-02-06T17:20:13.026+0000] {subprocess.py:93} INFO - 	at org.apache.hadoop.fs.statistics.impl.IOStatisticsBinding.lambda$trackDurationOfOperation$5(IOStatisticsBinding.java:499) ~[ex02-ingestion-assembly.jar:1.0.0]
[2026-02-06T17:20:13.026+0000] {subprocess.py:93} INFO - 	at org.apache.hadoop.fs.statistics.impl.IOStatisticsBinding.trackDuration(IOStatisticsBinding.java:444) ~[ex02-ingestion-assembly.jar:1.0.0]
[2026-02-06T17:20:13.026+0000] {subprocess.py:93} INFO - 	at org.apache.hadoop.fs.s3a.S3AFileSystem.trackDurationAndSpan(S3AFileSystem.java:2337) ~[ex02-ingestion-assembly.jar:1.0.0]
[2026-02-06T17:20:13.027+0000] {subprocess.py:93} INFO - 	at org.apache.hadoop.fs.s3a.S3AFileSystem.trackDurationAndSpan(S3AFileSystem.java:2356) ~[ex02-ingestion-assembly.jar:1.0.0]
[2026-02-06T17:20:13.027+0000] {subprocess.py:93} INFO - 	at org.apache.hadoop.fs.s3a.S3AFileSystem.create(S3AFileSystem.java:1645) ~[ex02-ingestion-assembly.jar:1.0.0]
[2026-02-06T17:20:13.027+0000] {subprocess.py:93} INFO - 	at org.apache.hadoop.fs.FileSystem.create(FileSystem.java:1195) ~[ex02-ingestion-assembly.jar:1.0.0]
[2026-02-06T17:20:13.027+0000] {subprocess.py:93} INFO - 	at org.apache.hadoop.fs.FileSystem.create(FileSystem.java:1175) ~[ex02-ingestion-assembly.jar:1.0.0]
[2026-02-06T17:20:13.027+0000] {subprocess.py:93} INFO - 	at org.apache.parquet.hadoop.util.HadoopOutputFile.create(HadoopOutputFile.java:74) ~[ex02-ingestion-assembly.jar:1.0.0]
[2026-02-06T17:20:13.027+0000] {subprocess.py:93} INFO - 	at org.apache.parquet.hadoop.ParquetFileWriter.<init>(ParquetFileWriter.java:347) ~[ex02-ingestion-assembly.jar:1.0.0]
[2026-02-06T17:20:13.027+0000] {subprocess.py:93} INFO - 	at org.apache.parquet.hadoop.ParquetFileWriter.<init>(ParquetFileWriter.java:314) ~[ex02-ingestion-assembly.jar:1.0.0]
[2026-02-06T17:20:13.027+0000] {subprocess.py:93} INFO - 	at org.apache.parquet.hadoop.ParquetOutputFormat.getRecordWriter(ParquetOutputFormat.java:484) ~[ex02-ingestion-assembly.jar:1.0.0]
[2026-02-06T17:20:13.027+0000] {subprocess.py:93} INFO - 	at org.apache.parquet.hadoop.ParquetOutputFormat.getRecordWriter(ParquetOutputFormat.java:422) ~[ex02-ingestion-assembly.jar:1.0.0]
[2026-02-06T17:20:13.027+0000] {subprocess.py:93} INFO - 	at org.apache.parquet.hadoop.ParquetOutputFormat.getRecordWriter(ParquetOutputFormat.java:411) ~[ex02-ingestion-assembly.jar:1.0.0]
[2026-02-06T17:20:13.028+0000] {subprocess.py:93} INFO - 	at org.apache.spark.sql.execution.datasources.parquet.ParquetOutputWriter.<init>(ParquetOutputWriter.scala:36) ~[ex02-ingestion-assembly.jar:1.0.0]
[2026-02-06T17:20:13.028+0000] {subprocess.py:93} INFO - 	at org.apache.spark.sql.execution.datasources.parquet.ParquetUtils$$anon$1.newInstance(ParquetUtils.scala:490) ~[ex02-ingestion-assembly.jar:1.0.0]
[2026-02-06T17:20:13.028+0000] {subprocess.py:93} INFO - 	at org.apache.spark.sql.execution.datasources.SingleDirectoryDataWriter.newOutputWriter(FileFormatDataWriter.scala:161) ~[ex02-ingestion-assembly.jar:1.0.0]
[2026-02-06T17:20:13.028+0000] {subprocess.py:93} INFO - 	at org.apache.spark.sql.execution.datasources.SingleDirectoryDataWriter.<init>(FileFormatDataWriter.scala:146) ~[ex02-ingestion-assembly.jar:1.0.0]
[2026-02-06T17:20:13.028+0000] {subprocess.py:93} INFO - 	at org.apache.spark.sql.execution.datasources.FileFormatWriter$.executeTask(FileFormatWriter.scala:389) ~[ex02-ingestion-assembly.jar:1.0.0]
[2026-02-06T17:20:13.028+0000] {subprocess.py:93} INFO - 	at org.apache.spark.sql.execution.datasources.WriteFilesExec.$anonfun$doExecuteWrite$1(WriteFiles.scala:100) ~[ex02-ingestion-assembly.jar:1.0.0]
[2026-02-06T17:20:13.028+0000] {subprocess.py:93} INFO - 	at org.apache.spark.rdd.RDD.$anonfun$mapPartitionsInternal$2(RDD.scala:893) ~[ex02-ingestion-assembly.jar:1.0.0]
[2026-02-06T17:20:13.028+0000] {subprocess.py:93} INFO - 	at org.apache.spark.rdd.RDD.$anonfun$mapPartitionsInternal$2$adapted(RDD.scala:893) ~[ex02-ingestion-assembly.jar:1.0.0]
[2026-02-06T17:20:13.028+0000] {subprocess.py:93} INFO - 	at org.apache.spark.rdd.MapPartitionsRDD.compute(MapPartitionsRDD.scala:52) ~[ex02-ingestion-assembly.jar:1.0.0]
[2026-02-06T17:20:13.029+0000] {subprocess.py:93} INFO - 	at org.apache.spark.rdd.RDD.computeOrReadCheckpoint(RDD.scala:367) ~[ex02-ingestion-assembly.jar:1.0.0]
[2026-02-06T17:20:13.029+0000] {subprocess.py:93} INFO - 	at org.apache.spark.rdd.RDD.iterator(RDD.scala:331) ~[ex02-ingestion-assembly.jar:1.0.0]
[2026-02-06T17:20:13.029+0000] {subprocess.py:93} INFO - 	at org.apache.spark.scheduler.ResultTask.runTask(ResultTask.scala:93) ~[ex02-ingestion-assembly.jar:1.0.0]
[2026-02-06T17:20:13.029+0000] {subprocess.py:93} INFO - 	at org.apache.spark.TaskContext.runTaskWithListeners(TaskContext.scala:166) ~[ex02-ingestion-assembly.jar:1.0.0]
[2026-02-06T17:20:13.029+0000] {subprocess.py:93} INFO - 	at org.apache.spark.scheduler.Task.run(Task.scala:141) ~[ex02-ingestion-assembly.jar:1.0.0]
[2026-02-06T17:20:13.029+0000] {subprocess.py:93} INFO - 	at org.apache.spark.executor.Executor$TaskRunner.$anonfun$run$4(Executor.scala:620) ~[ex02-ingestion-assembly.jar:1.0.0]
[2026-02-06T17:20:13.029+0000] {subprocess.py:93} INFO - 	at org.apache.spark.util.SparkErrorUtils.tryWithSafeFinally(SparkErrorUtils.scala:64) ~[ex02-ingestion-assembly.jar:1.0.0]
[2026-02-06T17:20:13.029+0000] {subprocess.py:93} INFO - 	at org.apache.spark.util.SparkErrorUtils.tryWithSafeFinally$(SparkErrorUtils.scala:61) ~[ex02-ingestion-assembly.jar:1.0.0]
[2026-02-06T17:20:13.029+0000] {subprocess.py:93} INFO - 	at org.apache.spark.util.Utils$.tryWithSafeFinally(Utils.scala:94) ~[ex02-ingestion-assembly.jar:1.0.0]
[2026-02-06T17:20:13.029+0000] {subprocess.py:93} INFO - 	at org.apache.spark.executor.Executor$TaskRunner.run(Executor.scala:623) ~[ex02-ingestion-assembly.jar:1.0.0]
[2026-02-06T17:20:13.030+0000] {subprocess.py:93} INFO - 	at java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1136) ~[?:?]
[2026-02-06T17:20:13.030+0000] {subprocess.py:93} INFO - 	at java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:635) ~[?:?]
[2026-02-06T17:20:13.030+0000] {subprocess.py:93} INFO - 	at java.lang.Thread.run(Thread.java:840) ~[?:?]
[2026-02-06T17:20:13.151+0000] {subprocess.py:93} INFO - Exception in thread "main" org.apache.spark.SparkException: Job aborted due to stage failure: Task 0 in stage 1.0 failed 1 times, most recent failure: Lost task 0.0 in stage 1.0 (TID 1) (4d21249f976f executor driver): org.apache.hadoop.fs.UnsupportedFileSystemException: No FileSystem for scheme "file"
[2026-02-06T17:20:13.152+0000] {subprocess.py:93} INFO - 	at org.apache.hadoop.fs.FileSystem.getFileSystemClass(FileSystem.java:3443)
[2026-02-06T17:20:13.152+0000] {subprocess.py:93} INFO - 	at org.apache.hadoop.fs.FileSystem.createFileSystem(FileSystem.java:3466)
[2026-02-06T17:20:13.152+0000] {subprocess.py:93} INFO - 	at org.apache.hadoop.fs.FileSystem.access$300(FileSystem.java:174)
[2026-02-06T17:20:13.152+0000] {subprocess.py:93} INFO - 	at org.apache.hadoop.fs.FileSystem$Cache.getInternal(FileSystem.java:3574)
[2026-02-06T17:20:13.152+0000] {subprocess.py:93} INFO - 	at org.apache.hadoop.fs.FileSystem$Cache.get(FileSystem.java:3521)
[2026-02-06T17:20:13.152+0000] {subprocess.py:93} INFO - 	at org.apache.hadoop.fs.FileSystem.get(FileSystem.java:540)
[2026-02-06T17:20:13.152+0000] {subprocess.py:93} INFO - 	at org.apache.hadoop.fs.FileSystem.getLocal(FileSystem.java:496)
[2026-02-06T17:20:13.152+0000] {subprocess.py:93} INFO - 	at org.apache.hadoop.fs.LocalDirAllocator$AllocatorPerContext.confChanged(LocalDirAllocator.java:316)
[2026-02-06T17:20:13.153+0000] {subprocess.py:93} INFO - 	at org.apache.hadoop.fs.LocalDirAllocator$AllocatorPerContext.getLocalPathForWrite(LocalDirAllocator.java:393)
[2026-02-06T17:20:13.153+0000] {subprocess.py:93} INFO - 	at org.apache.hadoop.fs.LocalDirAllocator.getLocalPathForWrite(LocalDirAllocator.java:165)
[2026-02-06T17:20:13.153+0000] {subprocess.py:93} INFO - 	at org.apache.hadoop.fs.LocalDirAllocator.getLocalPathForWrite(LocalDirAllocator.java:146)
[2026-02-06T17:20:13.153+0000] {subprocess.py:93} INFO - 	at org.apache.hadoop.fs.s3a.S3AFileSystem.createTmpFileForWrite(S3AFileSystem.java:1282)
[2026-02-06T17:20:13.153+0000] {subprocess.py:93} INFO - 	at org.apache.hadoop.fs.s3a.S3ADataBlocks$DiskBlockFactory.create(S3ADataBlocks.java:816)
[2026-02-06T17:20:13.153+0000] {subprocess.py:93} INFO - 	at org.apache.hadoop.fs.s3a.S3ABlockOutputStream.createBlockIfNeeded(S3ABlockOutputStream.java:211)
[2026-02-06T17:20:13.153+0000] {subprocess.py:93} INFO - 	at org.apache.hadoop.fs.s3a.S3ABlockOutputStream.<init>(S3ABlockOutputStream.java:188)
[2026-02-06T17:20:13.153+0000] {subprocess.py:93} INFO - 	at org.apache.hadoop.fs.s3a.S3AFileSystem.innerCreateFile(S3AFileSystem.java:1727)
[2026-02-06T17:20:13.153+0000] {subprocess.py:93} INFO - 	at org.apache.hadoop.fs.s3a.S3AFileSystem.lambda$create$6(S3AFileSystem.java:1646)
[2026-02-06T17:20:13.153+0000] {subprocess.py:93} INFO - 	at org.apache.hadoop.fs.statistics.impl.IOStatisticsBinding.lambda$trackDurationOfOperation$5(IOStatisticsBinding.java:499)
[2026-02-06T17:20:13.153+0000] {subprocess.py:93} INFO - 	at org.apache.hadoop.fs.statistics.impl.IOStatisticsBinding.trackDuration(IOStatisticsBinding.java:444)
[2026-02-06T17:20:13.153+0000] {subprocess.py:93} INFO - 	at org.apache.hadoop.fs.s3a.S3AFileSystem.trackDurationAndSpan(S3AFileSystem.java:2337)
[2026-02-06T17:20:13.153+0000] {subprocess.py:93} INFO - 	at org.apache.hadoop.fs.s3a.S3AFileSystem.trackDurationAndSpan(S3AFileSystem.java:2356)
[2026-02-06T17:20:13.153+0000] {subprocess.py:93} INFO - 	at org.apache.hadoop.fs.s3a.S3AFileSystem.create(S3AFileSystem.java:1645)
[2026-02-06T17:20:13.153+0000] {subprocess.py:93} INFO - 	at org.apache.hadoop.fs.FileSystem.create(FileSystem.java:1195)
[2026-02-06T17:20:13.153+0000] {subprocess.py:93} INFO - 	at org.apache.hadoop.fs.FileSystem.create(FileSystem.java:1175)
[2026-02-06T17:20:13.153+0000] {subprocess.py:93} INFO - 	at org.apache.parquet.hadoop.util.HadoopOutputFile.create(HadoopOutputFile.java:74)
[2026-02-06T17:20:13.154+0000] {subprocess.py:93} INFO - 	at org.apache.parquet.hadoop.ParquetFileWriter.<init>(ParquetFileWriter.java:347)
[2026-02-06T17:20:13.154+0000] {subprocess.py:93} INFO - 	at org.apache.parquet.hadoop.ParquetFileWriter.<init>(ParquetFileWriter.java:314)
[2026-02-06T17:20:13.154+0000] {subprocess.py:93} INFO - 	at org.apache.parquet.hadoop.ParquetOutputFormat.getRecordWriter(ParquetOutputFormat.java:484)
[2026-02-06T17:20:13.154+0000] {subprocess.py:93} INFO - 	at org.apache.parquet.hadoop.ParquetOutputFormat.getRecordWriter(ParquetOutputFormat.java:422)
[2026-02-06T17:20:13.154+0000] {subprocess.py:93} INFO - 	at org.apache.parquet.hadoop.ParquetOutputFormat.getRecordWriter(ParquetOutputFormat.java:411)
[2026-02-06T17:20:13.154+0000] {subprocess.py:93} INFO - 	at org.apache.spark.sql.execution.datasources.parquet.ParquetOutputWriter.<init>(ParquetOutputWriter.scala:36)
[2026-02-06T17:20:13.154+0000] {subprocess.py:93} INFO - 	at org.apache.spark.sql.execution.datasources.parquet.ParquetUtils$$anon$1.newInstance(ParquetUtils.scala:490)
[2026-02-06T17:20:13.154+0000] {subprocess.py:93} INFO - 	at org.apache.spark.sql.execution.datasources.SingleDirectoryDataWriter.newOutputWriter(FileFormatDataWriter.scala:161)
[2026-02-06T17:20:13.154+0000] {subprocess.py:93} INFO - 	at org.apache.spark.sql.execution.datasources.SingleDirectoryDataWriter.<init>(FileFormatDataWriter.scala:146)
[2026-02-06T17:20:13.154+0000] {subprocess.py:93} INFO - 	at org.apache.spark.sql.execution.datasources.FileFormatWriter$.executeTask(FileFormatWriter.scala:389)
[2026-02-06T17:20:13.154+0000] {subprocess.py:93} INFO - 	at org.apache.spark.sql.execution.datasources.WriteFilesExec.$anonfun$doExecuteWrite$1(WriteFiles.scala:100)
[2026-02-06T17:20:13.154+0000] {subprocess.py:93} INFO - 	at org.apache.spark.rdd.RDD.$anonfun$mapPartitionsInternal$2(RDD.scala:893)
[2026-02-06T17:20:13.154+0000] {subprocess.py:93} INFO - 	at org.apache.spark.rdd.RDD.$anonfun$mapPartitionsInternal$2$adapted(RDD.scala:893)
[2026-02-06T17:20:13.154+0000] {subprocess.py:93} INFO - 	at org.apache.spark.rdd.MapPartitionsRDD.compute(MapPartitionsRDD.scala:52)
[2026-02-06T17:20:13.154+0000] {subprocess.py:93} INFO - 	at org.apache.spark.rdd.RDD.computeOrReadCheckpoint(RDD.scala:367)
[2026-02-06T17:20:13.154+0000] {subprocess.py:93} INFO - 	at org.apache.spark.rdd.RDD.iterator(RDD.scala:331)
[2026-02-06T17:20:13.154+0000] {subprocess.py:93} INFO - 	at org.apache.spark.scheduler.ResultTask.runTask(ResultTask.scala:93)
[2026-02-06T17:20:13.154+0000] {subprocess.py:93} INFO - 	at org.apache.spark.TaskContext.runTaskWithListeners(TaskContext.scala:166)
[2026-02-06T17:20:13.154+0000] {subprocess.py:93} INFO - 	at org.apache.spark.scheduler.Task.run(Task.scala:141)
[2026-02-06T17:20:13.155+0000] {subprocess.py:93} INFO - 	at org.apache.spark.executor.Executor$TaskRunner.$anonfun$run$4(Executor.scala:620)
[2026-02-06T17:20:13.155+0000] {subprocess.py:93} INFO - 	at org.apache.spark.util.SparkErrorUtils.tryWithSafeFinally(SparkErrorUtils.scala:64)
[2026-02-06T17:20:13.155+0000] {subprocess.py:93} INFO - 	at org.apache.spark.util.SparkErrorUtils.tryWithSafeFinally$(SparkErrorUtils.scala:61)
[2026-02-06T17:20:13.155+0000] {subprocess.py:93} INFO - 	at org.apache.spark.util.Utils$.tryWithSafeFinally(Utils.scala:94)
[2026-02-06T17:20:13.155+0000] {subprocess.py:93} INFO - 	at org.apache.spark.executor.Executor$TaskRunner.run(Executor.scala:623)
[2026-02-06T17:20:13.155+0000] {subprocess.py:93} INFO - 	at java.base/java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1136)
[2026-02-06T17:20:13.155+0000] {subprocess.py:93} INFO - 	at java.base/java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:635)
[2026-02-06T17:20:13.155+0000] {subprocess.py:93} INFO - 	at java.base/java.lang.Thread.run(Thread.java:840)
[2026-02-06T17:20:13.155+0000] {subprocess.py:93} INFO - 
[2026-02-06T17:20:13.155+0000] {subprocess.py:93} INFO - Driver stacktrace:
[2026-02-06T17:20:13.155+0000] {subprocess.py:93} INFO - 	at org.apache.spark.scheduler.DAGScheduler.failJobAndIndependentStages(DAGScheduler.scala:2856)
[2026-02-06T17:20:13.155+0000] {subprocess.py:93} INFO - 	at org.apache.spark.scheduler.DAGScheduler.$anonfun$abortStage$2(DAGScheduler.scala:2792)
[2026-02-06T17:20:13.155+0000] {subprocess.py:93} INFO - 	at org.apache.spark.scheduler.DAGScheduler.$anonfun$abortStage$2$adapted(DAGScheduler.scala:2791)
[2026-02-06T17:20:13.155+0000] {subprocess.py:93} INFO - 	at scala.collection.immutable.List.foreach(List.scala:323)
[2026-02-06T17:20:13.155+0000] {subprocess.py:93} INFO - 	at org.apache.spark.scheduler.DAGScheduler.abortStage(DAGScheduler.scala:2791)
[2026-02-06T17:20:13.155+0000] {subprocess.py:93} INFO - 	at org.apache.spark.scheduler.DAGScheduler.$anonfun$handleTaskSetFailed$1(DAGScheduler.scala:1247)
[2026-02-06T17:20:13.155+0000] {subprocess.py:93} INFO - 	at org.apache.spark.scheduler.DAGScheduler.$anonfun$handleTaskSetFailed$1$adapted(DAGScheduler.scala:1247)
[2026-02-06T17:20:13.155+0000] {subprocess.py:93} INFO - 	at scala.Option.foreach(Option.scala:437)
[2026-02-06T17:20:13.156+0000] {subprocess.py:93} INFO - 	at org.apache.spark.scheduler.DAGScheduler.handleTaskSetFailed(DAGScheduler.scala:1247)
[2026-02-06T17:20:13.156+0000] {subprocess.py:93} INFO - 	at org.apache.spark.scheduler.DAGSchedulerEventProcessLoop.doOnReceive(DAGScheduler.scala:3060)
[2026-02-06T17:20:13.156+0000] {subprocess.py:93} INFO - 	at org.apache.spark.scheduler.DAGSchedulerEventProcessLoop.onReceive(DAGScheduler.scala:2994)
[2026-02-06T17:20:13.156+0000] {subprocess.py:93} INFO - 	at org.apache.spark.scheduler.DAGSchedulerEventProcessLoop.onReceive(DAGScheduler.scala:2983)
[2026-02-06T17:20:13.156+0000] {subprocess.py:93} INFO - 	at org.apache.spark.util.EventLoop$$anon$1.run(EventLoop.scala:49)
[2026-02-06T17:20:13.156+0000] {subprocess.py:93} INFO - 	at org.apache.spark.scheduler.DAGScheduler.runJob(DAGScheduler.scala:989)
[2026-02-06T17:20:13.156+0000] {subprocess.py:93} INFO - 	at org.apache.spark.SparkContext.runJob(SparkContext.scala:2393)
[2026-02-06T17:20:13.156+0000] {subprocess.py:93} INFO - 	at org.apache.spark.sql.execution.datasources.FileFormatWriter$.$anonfun$executeWrite$4(FileFormatWriter.scala:307)
[2026-02-06T17:20:13.156+0000] {subprocess.py:93} INFO - 	at org.apache.spark.sql.execution.datasources.FileFormatWriter$.writeAndCommit(FileFormatWriter.scala:271)
[2026-02-06T17:20:13.156+0000] {subprocess.py:93} INFO - 	at org.apache.spark.sql.execution.datasources.FileFormatWriter$.executeWrite(FileFormatWriter.scala:304)
[2026-02-06T17:20:13.156+0000] {subprocess.py:93} INFO - 	at org.apache.spark.sql.execution.datasources.FileFormatWriter$.write(FileFormatWriter.scala:190)
[2026-02-06T17:20:13.156+0000] {subprocess.py:93} INFO - 	at org.apache.spark.sql.execution.datasources.InsertIntoHadoopFsRelationCommand.run(InsertIntoHadoopFsRelationCommand.scala:190)
[2026-02-06T17:20:13.156+0000] {subprocess.py:93} INFO - 	at org.apache.spark.sql.execution.command.DataWritingCommandExec.sideEffectResult$lzycompute(commands.scala:113)
[2026-02-06T17:20:13.156+0000] {subprocess.py:93} INFO - 	at org.apache.spark.sql.execution.command.DataWritingCommandExec.sideEffectResult(commands.scala:111)
[2026-02-06T17:20:13.156+0000] {subprocess.py:93} INFO - 	at org.apache.spark.sql.execution.command.DataWritingCommandExec.executeCollect(commands.scala:125)
[2026-02-06T17:20:13.156+0000] {subprocess.py:93} INFO - 	at org.apache.spark.sql.execution.QueryExecution$$anonfun$eagerlyExecuteCommands$1.$anonfun$applyOrElse$1(QueryExecution.scala:107)
[2026-02-06T17:20:13.156+0000] {subprocess.py:93} INFO - 	at org.apache.spark.sql.execution.SQLExecution$.$anonfun$withNewExecutionId$6(SQLExecution.scala:125)
[2026-02-06T17:20:13.156+0000] {subprocess.py:93} INFO - 	at org.apache.spark.sql.execution.SQLExecution$.withSQLConfPropagated(SQLExecution.scala:201)
[2026-02-06T17:20:13.156+0000] {subprocess.py:93} INFO - 	at org.apache.spark.sql.execution.SQLExecution$.$anonfun$withNewExecutionId$1(SQLExecution.scala:108)
[2026-02-06T17:20:13.157+0000] {subprocess.py:93} INFO - 	at org.apache.spark.sql.SparkSession.withActive(SparkSession.scala:900)
[2026-02-06T17:20:13.157+0000] {subprocess.py:93} INFO - 	at org.apache.spark.sql.execution.SQLExecution$.withNewExecutionId(SQLExecution.scala:66)
[2026-02-06T17:20:13.157+0000] {subprocess.py:93} INFO - 	at org.apache.spark.sql.execution.QueryExecution$$anonfun$eagerlyExecuteCommands$1.applyOrElse(QueryExecution.scala:107)
[2026-02-06T17:20:13.157+0000] {subprocess.py:93} INFO - 	at org.apache.spark.sql.execution.QueryExecution$$anonfun$eagerlyExecuteCommands$1.applyOrElse(QueryExecution.scala:98)
[2026-02-06T17:20:13.157+0000] {subprocess.py:93} INFO - 	at org.apache.spark.sql.catalyst.trees.TreeNode.$anonfun$transformDownWithPruning$1(TreeNode.scala:461)
[2026-02-06T17:20:13.157+0000] {subprocess.py:93} INFO - 	at org.apache.spark.sql.catalyst.trees.CurrentOrigin$.withOrigin(origin.scala:76)
[2026-02-06T17:20:13.157+0000] {subprocess.py:93} INFO - 	at org.apache.spark.sql.catalyst.trees.TreeNode.transformDownWithPruning(TreeNode.scala:461)
[2026-02-06T17:20:13.157+0000] {subprocess.py:93} INFO - 	at org.apache.spark.sql.catalyst.plans.logical.LogicalPlan.org$apache$spark$sql$catalyst$plans$logical$AnalysisHelper$$super$transformDownWithPruning(LogicalPlan.scala:32)
[2026-02-06T17:20:13.157+0000] {subprocess.py:93} INFO - 	at org.apache.spark.sql.catalyst.plans.logical.AnalysisHelper.transformDownWithPruning(AnalysisHelper.scala:267)
[2026-02-06T17:20:13.157+0000] {subprocess.py:93} INFO - 	at org.apache.spark.sql.catalyst.plans.logical.AnalysisHelper.transformDownWithPruning$(AnalysisHelper.scala:263)
[2026-02-06T17:20:13.157+0000] {subprocess.py:93} INFO - 	at org.apache.spark.sql.catalyst.plans.logical.LogicalPlan.transformDownWithPruning(LogicalPlan.scala:32)
[2026-02-06T17:20:13.157+0000] {subprocess.py:93} INFO - 	at org.apache.spark.sql.catalyst.plans.logical.LogicalPlan.transformDownWithPruning(LogicalPlan.scala:32)
[2026-02-06T17:20:13.157+0000] {subprocess.py:93} INFO - 	at org.apache.spark.sql.catalyst.trees.TreeNode.transformDown(TreeNode.scala:437)
[2026-02-06T17:20:13.157+0000] {subprocess.py:93} INFO - 	at org.apache.spark.sql.execution.QueryExecution.eagerlyExecuteCommands(QueryExecution.scala:98)
[2026-02-06T17:20:13.157+0000] {subprocess.py:93} INFO - 	at org.apache.spark.sql.execution.QueryExecution.commandExecuted$lzycompute(QueryExecution.scala:85)
[2026-02-06T17:20:13.157+0000] {subprocess.py:93} INFO - 	at org.apache.spark.sql.execution.QueryExecution.commandExecuted(QueryExecution.scala:83)
[2026-02-06T17:20:13.157+0000] {subprocess.py:93} INFO - 	at org.apache.spark.sql.execution.QueryExecution.assertCommandExecuted(QueryExecution.scala:142)
[2026-02-06T17:20:13.157+0000] {subprocess.py:93} INFO - 	at org.apache.spark.sql.DataFrameWriter.runCommand(DataFrameWriter.scala:869)
[2026-02-06T17:20:13.157+0000] {subprocess.py:93} INFO - 	at org.apache.spark.sql.DataFrameWriter.saveToV1Source(DataFrameWriter.scala:391)
[2026-02-06T17:20:13.158+0000] {subprocess.py:93} INFO - 	at org.apache.spark.sql.DataFrameWriter.saveInternal(DataFrameWriter.scala:364)
[2026-02-06T17:20:13.158+0000] {subprocess.py:93} INFO - 	at org.apache.spark.sql.DataFrameWriter.save(DataFrameWriter.scala:243)
[2026-02-06T17:20:13.158+0000] {subprocess.py:93} INFO - 	at org.apache.spark.sql.DataFrameWriter.parquet(DataFrameWriter.scala:802)
[2026-02-06T17:20:13.158+0000] {subprocess.py:93} INFO - 	at fr.cytech.ingestion.Main$.main(Main.scala:107)
[2026-02-06T17:20:13.158+0000] {subprocess.py:93} INFO - 	at fr.cytech.ingestion.Main.main(Main.scala)
[2026-02-06T17:20:13.158+0000] {subprocess.py:93} INFO - Caused by: org.apache.hadoop.fs.UnsupportedFileSystemException: No FileSystem for scheme "file"
[2026-02-06T17:20:13.158+0000] {subprocess.py:93} INFO - 	at org.apache.hadoop.fs.FileSystem.getFileSystemClass(FileSystem.java:3443)
[2026-02-06T17:20:13.158+0000] {subprocess.py:93} INFO - 	at org.apache.hadoop.fs.FileSystem.createFileSystem(FileSystem.java:3466)
[2026-02-06T17:20:13.158+0000] {subprocess.py:93} INFO - 	at org.apache.hadoop.fs.FileSystem.access$300(FileSystem.java:174)
[2026-02-06T17:20:13.158+0000] {subprocess.py:93} INFO - 	at org.apache.hadoop.fs.FileSystem$Cache.getInternal(FileSystem.java:3574)
[2026-02-06T17:20:13.158+0000] {subprocess.py:93} INFO - 	at org.apache.hadoop.fs.FileSystem$Cache.get(FileSystem.java:3521)
[2026-02-06T17:20:13.158+0000] {subprocess.py:93} INFO - 	at org.apache.hadoop.fs.FileSystem.get(FileSystem.java:540)
[2026-02-06T17:20:13.158+0000] {subprocess.py:93} INFO - 	at org.apache.hadoop.fs.FileSystem.getLocal(FileSystem.java:496)
[2026-02-06T17:20:13.158+0000] {subprocess.py:93} INFO - 	at org.apache.hadoop.fs.LocalDirAllocator$AllocatorPerContext.confChanged(LocalDirAllocator.java:316)
[2026-02-06T17:20:13.158+0000] {subprocess.py:93} INFO - 	at org.apache.hadoop.fs.LocalDirAllocator$AllocatorPerContext.getLocalPathForWrite(LocalDirAllocator.java:393)
[2026-02-06T17:20:13.158+0000] {subprocess.py:93} INFO - 	at org.apache.hadoop.fs.LocalDirAllocator.getLocalPathForWrite(LocalDirAllocator.java:165)
[2026-02-06T17:20:13.158+0000] {subprocess.py:93} INFO - 	at org.apache.hadoop.fs.LocalDirAllocator.getLocalPathForWrite(LocalDirAllocator.java:146)
[2026-02-06T17:20:13.158+0000] {subprocess.py:93} INFO - 	at org.apache.hadoop.fs.s3a.S3AFileSystem.createTmpFileForWrite(S3AFileSystem.java:1282)
[2026-02-06T17:20:13.158+0000] {subprocess.py:93} INFO - 	at org.apache.hadoop.fs.s3a.S3ADataBlocks$DiskBlockFactory.create(S3ADataBlocks.java:816)
[2026-02-06T17:20:13.158+0000] {subprocess.py:93} INFO - 	at org.apache.hadoop.fs.s3a.S3ABlockOutputStream.createBlockIfNeeded(S3ABlockOutputStream.java:211)
[2026-02-06T17:20:13.158+0000] {subprocess.py:93} INFO - 	at org.apache.hadoop.fs.s3a.S3ABlockOutputStream.<init>(S3ABlockOutputStream.java:188)
[2026-02-06T17:20:13.159+0000] {subprocess.py:93} INFO - 	at org.apache.hadoop.fs.s3a.S3AFileSystem.innerCreateFile(S3AFileSystem.java:1727)
[2026-02-06T17:20:13.159+0000] {subprocess.py:93} INFO - 	at org.apache.hadoop.fs.s3a.S3AFileSystem.lambda$create$6(S3AFileSystem.java:1646)
[2026-02-06T17:20:13.159+0000] {subprocess.py:93} INFO - 	at org.apache.hadoop.fs.statistics.impl.IOStatisticsBinding.lambda$trackDurationOfOperation$5(IOStatisticsBinding.java:499)
[2026-02-06T17:20:13.159+0000] {subprocess.py:93} INFO - 	at org.apache.hadoop.fs.statistics.impl.IOStatisticsBinding.trackDuration(IOStatisticsBinding.java:444)
[2026-02-06T17:20:13.159+0000] {subprocess.py:93} INFO - 	at org.apache.hadoop.fs.s3a.S3AFileSystem.trackDurationAndSpan(S3AFileSystem.java:2337)
[2026-02-06T17:20:13.159+0000] {subprocess.py:93} INFO - 	at org.apache.hadoop.fs.s3a.S3AFileSystem.trackDurationAndSpan(S3AFileSystem.java:2356)
[2026-02-06T17:20:13.159+0000] {subprocess.py:93} INFO - 	at org.apache.hadoop.fs.s3a.S3AFileSystem.create(S3AFileSystem.java:1645)
[2026-02-06T17:20:13.159+0000] {subprocess.py:93} INFO - 	at org.apache.hadoop.fs.FileSystem.create(FileSystem.java:1195)
[2026-02-06T17:20:13.159+0000] {subprocess.py:93} INFO - 	at org.apache.hadoop.fs.FileSystem.create(FileSystem.java:1175)
[2026-02-06T17:20:13.159+0000] {subprocess.py:93} INFO - 	at org.apache.parquet.hadoop.util.HadoopOutputFile.create(HadoopOutputFile.java:74)
[2026-02-06T17:20:13.159+0000] {subprocess.py:93} INFO - 	at org.apache.parquet.hadoop.ParquetFileWriter.<init>(ParquetFileWriter.java:347)
[2026-02-06T17:20:13.159+0000] {subprocess.py:93} INFO - 	at org.apache.parquet.hadoop.ParquetFileWriter.<init>(ParquetFileWriter.java:314)
[2026-02-06T17:20:13.159+0000] {subprocess.py:93} INFO - 	at org.apache.parquet.hadoop.ParquetOutputFormat.getRecordWriter(ParquetOutputFormat.java:484)
[2026-02-06T17:20:13.159+0000] {subprocess.py:93} INFO - 	at org.apache.parquet.hadoop.ParquetOutputFormat.getRecordWriter(ParquetOutputFormat.java:422)
[2026-02-06T17:20:13.159+0000] {subprocess.py:93} INFO - 	at org.apache.parquet.hadoop.ParquetOutputFormat.getRecordWriter(ParquetOutputFormat.java:411)
[2026-02-06T17:20:13.159+0000] {subprocess.py:93} INFO - 	at org.apache.spark.sql.execution.datasources.parquet.ParquetOutputWriter.<init>(ParquetOutputWriter.scala:36)
[2026-02-06T17:20:13.159+0000] {subprocess.py:93} INFO - 	at org.apache.spark.sql.execution.datasources.parquet.ParquetUtils$$anon$1.newInstance(ParquetUtils.scala:490)
[2026-02-06T17:20:13.159+0000] {subprocess.py:93} INFO - 	at org.apache.spark.sql.execution.datasources.SingleDirectoryDataWriter.newOutputWriter(FileFormatDataWriter.scala:161)
[2026-02-06T17:20:13.159+0000] {subprocess.py:93} INFO - 	at org.apache.spark.sql.execution.datasources.SingleDirectoryDataWriter.<init>(FileFormatDataWriter.scala:146)
[2026-02-06T17:20:13.160+0000] {subprocess.py:93} INFO - 	at org.apache.spark.sql.execution.datasources.FileFormatWriter$.executeTask(FileFormatWriter.scala:389)
[2026-02-06T17:20:13.160+0000] {subprocess.py:93} INFO - 	at org.apache.spark.sql.execution.datasources.WriteFilesExec.$anonfun$doExecuteWrite$1(WriteFiles.scala:100)
[2026-02-06T17:20:13.160+0000] {subprocess.py:93} INFO - 	at org.apache.spark.rdd.RDD.$anonfun$mapPartitionsInternal$2(RDD.scala:893)
[2026-02-06T17:20:13.160+0000] {subprocess.py:93} INFO - 	at org.apache.spark.rdd.RDD.$anonfun$mapPartitionsInternal$2$adapted(RDD.scala:893)
[2026-02-06T17:20:13.160+0000] {subprocess.py:93} INFO - 	at org.apache.spark.rdd.MapPartitionsRDD.compute(MapPartitionsRDD.scala:52)
[2026-02-06T17:20:13.160+0000] {subprocess.py:93} INFO - 	at org.apache.spark.rdd.RDD.computeOrReadCheckpoint(RDD.scala:367)
[2026-02-06T17:20:13.160+0000] {subprocess.py:93} INFO - 	at org.apache.spark.rdd.RDD.iterator(RDD.scala:331)
[2026-02-06T17:20:13.160+0000] {subprocess.py:93} INFO - 	at org.apache.spark.scheduler.ResultTask.runTask(ResultTask.scala:93)
[2026-02-06T17:20:13.160+0000] {subprocess.py:93} INFO - 	at org.apache.spark.TaskContext.runTaskWithListeners(TaskContext.scala:166)
[2026-02-06T17:20:13.160+0000] {subprocess.py:93} INFO - 	at org.apache.spark.scheduler.Task.run(Task.scala:141)
[2026-02-06T17:20:13.160+0000] {subprocess.py:93} INFO - 	at org.apache.spark.executor.Executor$TaskRunner.$anonfun$run$4(Executor.scala:620)
[2026-02-06T17:20:13.160+0000] {subprocess.py:93} INFO - 	at org.apache.spark.util.SparkErrorUtils.tryWithSafeFinally(SparkErrorUtils.scala:64)
[2026-02-06T17:20:13.160+0000] {subprocess.py:93} INFO - 	at org.apache.spark.util.SparkErrorUtils.tryWithSafeFinally$(SparkErrorUtils.scala:61)
[2026-02-06T17:20:13.160+0000] {subprocess.py:93} INFO - 	at org.apache.spark.util.Utils$.tryWithSafeFinally(Utils.scala:94)
[2026-02-06T17:20:13.160+0000] {subprocess.py:93} INFO - 	at org.apache.spark.executor.Executor$TaskRunner.run(Executor.scala:623)
[2026-02-06T17:20:13.160+0000] {subprocess.py:93} INFO - 	at java.base/java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1136)
[2026-02-06T17:20:13.160+0000] {subprocess.py:93} INFO - 	at java.base/java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:635)
[2026-02-06T17:20:13.160+0000] {subprocess.py:93} INFO - 	at java.base/java.lang.Thread.run(Thread.java:840)
[2026-02-06T17:21:09.884+0000] {subprocess.py:97} INFO - Command exited with return code 1
[2026-02-06T17:21:09.890+0000] {taskinstance.py:3310} ERROR - Task failed with exception
Traceback (most recent call last):
  File "/home/airflow/.local/lib/python3.12/site-packages/airflow/models/taskinstance.py", line 767, in _execute_task
    result = _execute_callable(context=context, **execute_callable_kwargs)
             ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/home/airflow/.local/lib/python3.12/site-packages/airflow/models/taskinstance.py", line 733, in _execute_callable
    return ExecutionCallableRunner(
           ^^^^^^^^^^^^^^^^^^^^^^^^
  File "/home/airflow/.local/lib/python3.12/site-packages/airflow/utils/operator_helpers.py", line 252, in run
    return self.func(*args, **kwargs)
           ^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/home/airflow/.local/lib/python3.12/site-packages/airflow/models/baseoperator.py", line 406, in wrapper
    return func(self, *args, **kwargs)
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/home/airflow/.local/lib/python3.12/site-packages/airflow/operators/bash.py", line 249, in execute
    raise AirflowException(
airflow.exceptions.AirflowException: Bash command failed. The command returned a non-zero exit code 1.
[2026-02-06T17:21:09.895+0000] {taskinstance.py:1225} INFO - Marking task as FAILED. dag_id=tp_bigdata_pipeline, task_id=ex02_ingestion, run_id=manual__2026-02-06T17:18:48.245383+00:00, execution_date=20260206T171848, start_date=20260206T172006, end_date=20260206T172109
[2026-02-06T17:21:09.911+0000] {taskinstance.py:340} INFO - ::group::Post task execution logs
[2026-02-06T17:21:09.911+0000] {standard_task_runner.py:124} ERROR - Failed to execute job 54 for task ex02_ingestion (Bash command failed. The command returned a non-zero exit code 1.; 730)
Traceback (most recent call last):
  File "/home/airflow/.local/lib/python3.12/site-packages/airflow/task/task_runner/standard_task_runner.py", line 117, in _start_by_fork
    ret = args.func(args, dag=self.dag)
          ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/home/airflow/.local/lib/python3.12/site-packages/airflow/cli/cli_config.py", line 49, in command
    return func(*args, **kwargs)
           ^^^^^^^^^^^^^^^^^^^^^
  File "/home/airflow/.local/lib/python3.12/site-packages/airflow/utils/cli.py", line 115, in wrapper
    return f(*args, **kwargs)
           ^^^^^^^^^^^^^^^^^^
  File "/home/airflow/.local/lib/python3.12/site-packages/airflow/cli/commands/task_command.py", line 483, in task_run
    task_return_code = _run_task_by_selected_method(args, _dag, ti)
                       ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/home/airflow/.local/lib/python3.12/site-packages/airflow/cli/commands/task_command.py", line 256, in _run_task_by_selected_method
    return _run_raw_task(args, ti)
           ^^^^^^^^^^^^^^^^^^^^^^^
  File "/home/airflow/.local/lib/python3.12/site-packages/airflow/cli/commands/task_command.py", line 341, in _run_raw_task
    return ti._run_raw_task(
           ^^^^^^^^^^^^^^^^^
  File "/home/airflow/.local/lib/python3.12/site-packages/airflow/utils/session.py", line 97, in wrapper
    return func(*args, session=session, **kwargs)
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/home/airflow/.local/lib/python3.12/site-packages/airflow/models/taskinstance.py", line 3004, in _run_raw_task
    return _run_raw_task(
           ^^^^^^^^^^^^^^
  File "/home/airflow/.local/lib/python3.12/site-packages/airflow/models/taskinstance.py", line 273, in _run_raw_task
    TaskInstance._execute_task_with_callbacks(
  File "/home/airflow/.local/lib/python3.12/site-packages/airflow/models/taskinstance.py", line 3158, in _execute_task_with_callbacks
    result = self._execute_task(context, task_orig)
             ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/home/airflow/.local/lib/python3.12/site-packages/airflow/models/taskinstance.py", line 3182, in _execute_task
    return _execute_task(self, context, task_orig)
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/home/airflow/.local/lib/python3.12/site-packages/airflow/models/taskinstance.py", line 767, in _execute_task
    result = _execute_callable(context=context, **execute_callable_kwargs)
             ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/home/airflow/.local/lib/python3.12/site-packages/airflow/models/taskinstance.py", line 733, in _execute_callable
    return ExecutionCallableRunner(
           ^^^^^^^^^^^^^^^^^^^^^^^^
  File "/home/airflow/.local/lib/python3.12/site-packages/airflow/utils/operator_helpers.py", line 252, in run
    return self.func(*args, **kwargs)
           ^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/home/airflow/.local/lib/python3.12/site-packages/airflow/models/baseoperator.py", line 406, in wrapper
    return func(self, *args, **kwargs)
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/home/airflow/.local/lib/python3.12/site-packages/airflow/operators/bash.py", line 249, in execute
    raise AirflowException(
airflow.exceptions.AirflowException: Bash command failed. The command returned a non-zero exit code 1.
[2026-02-06T17:21:09.928+0000] {local_task_job_runner.py:266} INFO - Task exited with return code 1
[2026-02-06T17:21:09.938+0000] {taskinstance.py:3900} INFO - 0 downstream tasks scheduled from follow-on schedule check
[2026-02-06T17:21:09.952+0000] {local_task_job_runner.py:245} INFO - ::endgroup::
